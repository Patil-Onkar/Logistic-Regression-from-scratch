{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J_TTq18unAV",
        "colab_type": "text"
      },
      "source": [
        "#Logistic Regression from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSoJYwgwuNMQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "646a7f0a-4074-4d4c-fa7d-2373a15b9943"
      },
      "source": [
        "#loading the data\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "iris = sns.load_dataset('iris')\n",
        "\n",
        "iris.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal_length  sepal_width  petal_length  petal_width species\n",
              "0           5.1          3.5           1.4          0.2  setosa\n",
              "1           4.9          3.0           1.4          0.2  setosa\n",
              "2           4.7          3.2           1.3          0.2  setosa\n",
              "3           4.6          3.1           1.5          0.2  setosa\n",
              "4           5.0          3.6           1.4          0.2  setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_JV-aDh3016",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "6790b73c-7422-4a19-b394-ff7ad47de234"
      },
      "source": [
        "iris.info()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   sepal_length  150 non-null    float64\n",
            " 1   sepal_width   150 non-null    float64\n",
            " 2   petal_length  150 non-null    float64\n",
            " 3   petal_width   150 non-null    float64\n",
            " 4   species       150 non-null    object \n",
            "dtypes: float64(4), object(1)\n",
            "memory usage: 6.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufoDwj51vKck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBNxjpcWvMbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into train and test\n",
        "# Here, Logistic regression is used to binary classification so we exclude one class.\n",
        "\n",
        "k=iris.groupby('species')\n",
        "trn=pd.DataFrame()\n",
        "tst=pd.DataFrame()\n",
        "p=0\n",
        "for g,i in k:\n",
        "  if g!='virginica':\n",
        "    trn=trn.append(i[:-10],ignore_index=True)\n",
        "    tst=tst.append(i[-10:],ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnBTAnz0wPwB",
        "colab_type": "text"
      },
      "source": [
        "*following are the key steps in implementing logistic regression*\n",
        "\n",
        "Applying Logistic regression algorithm - \n",
        "\n",
        "1. Calculate the logits : a1x1+a2x2+a3x3+a4x4+a (As we have 4 attributes a for bias)\n",
        "\n",
        "2. Apply sigmoid function : 1/1+e-z\n",
        "\n",
        "3. Compute the loss function\n",
        "\n",
        "4. Implementing Gradient Discent "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95AywqQtJXNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Class is defined to implement logistic regression. Specifically for Iris dataset.\n",
        "\n",
        "Class consists methods to train the data and measure the accuracy\n",
        "\n",
        "'''\n",
        "\n",
        "class Logistic_Regression():\n",
        "  def __init__(self,trn,tst):\n",
        "    self.a1=1.0\n",
        "    self.a2=1.0\n",
        "    self.a3=1.0\n",
        "    self.a4=1.0\n",
        "    self.a=2.0\n",
        "    self.trn=trn\n",
        "    self.tst=tst\n",
        "    self.trnx,self.trny,self.tstx,self.tsty=self.pre_processing(trn,tst)\n",
        "    self.lr=0.000005\n",
        "\n",
        " # This method (pre-processing) converts classes into 0 and 1.\n",
        " # This method takes the training and test data and splits it, train data into atributes (trnx) and class (trny) similarly for test data and return it\n",
        "\n",
        "  def pre_processing(self,trn,tst):\n",
        "    n=len(trn.columns)\n",
        "    attr=list(trn.columns[0:n-1])\n",
        "    trnx=trn[attr]\n",
        "    trny=trn[trn.columns[-1]]\n",
        "    trny=trny.map({'setosa':1.0,'versicolor':0.0})\n",
        "    tstx=tst[attr]\n",
        "    tsty=tst[trn.columns[-1]]\n",
        "    tsty=tsty.map({'setosa':1.0,'versicolor':0.0})\n",
        "    return trnx,trny,tstx,tsty\n",
        "\n",
        "# Following method takes train(attribute) data and its corresponing classes as input and calculate cross entropy loss associate with it  \n",
        "\n",
        "  def Loss(self,trnx,trny):\n",
        "    atr=list(trnx.columns)\n",
        "    logits=self.a1*trnx[atr[0]]+self.a2*trnx[atr[1]]+self.a3*trnx[atr[2]]+self.a4*trnx[atr[3]]+self.a\n",
        "    z=1/(1+np.exp(-np.array(logits)))\n",
        "    l=sum(trny*np.log(z+0.000001)+(1-trny)*np.log(1.000001-z))\n",
        "    return -l\n",
        "\n",
        "# Following method once called applies Gradient descent method and reduce the loss and iterate untill it achieves loss value \n",
        "# less than 0.01. \n",
        "\n",
        "  def Train(self):\n",
        "    trnx=self.trnx\n",
        "    trny=self.trny\n",
        "    atr=list(trnx.columns)\n",
        "    dL=1.0\n",
        "    cnt=0\n",
        "    while(dL>0.01):\n",
        "      loss1=self.Loss(trnx,trny)\n",
        "      #print('a')\n",
        "      logits=self.a1*trnx[atr[0]]+self.a2*trnx[atr[1]]+self.a3*trnx[atr[2]]+self.a4*trnx[atr[3]]+self.a\n",
        "      z=1/(1+np.exp(-np.array(logits)))+0.0000001\n",
        "      dz=z*(1-z)\n",
        "      dcrs=np.array(trny)*(1/z)+(1-np.array(trny))*(1/(z-1))\n",
        "      self.a1=self.a1+self.lr*sum(dcrs*dz*np.array(trnx[atr[0]]))\n",
        "      logits=self.a1*trnx[atr[0]]+self.a2*trnx[atr[1]]+self.a3*trnx[atr[2]]+self.a4*trnx[atr[3]]+self.a\n",
        "      z=1/(1+np.exp(-np.array(logits)))+0.0000001\n",
        "      dz=z*(1-z)\n",
        "      dcrs=np.array(trny)*(1/z)+(1-np.array(trny))*(1/(z-1))\n",
        "      self.a2=self.a2+self.lr*sum(dcrs*dz*np.array(trnx[atr[1]]))\n",
        "      logits=self.a1*trnx[atr[0]]+self.a2*trnx[atr[1]]+self.a3*trnx[atr[2]]+self.a4*trnx[atr[3]]+self.a\n",
        "      z=1/(1+np.exp(-np.array(logits)))+0.0000001\n",
        "      dz=z*(1-z)\n",
        "      dcrs=np.array(trny)*(1/z)+(1-np.array(trny))*(1/(z-1))\n",
        "      self.a3=self.a3+self.lr*sum(dcrs*dz*np.array(trnx[atr[2]]))\n",
        "      logits=self.a1*trnx[atr[0]]+self.a2*trnx[atr[1]]+self.a3*trnx[atr[2]]+self.a4*trnx[atr[3]]+self.a\n",
        "      z=1/(1+np.exp(-np.array(logits)))+0.0000001\n",
        "      dz=z*(1-z)\n",
        "      dcrs=np.array(trny)*(1/z)+(1-np.array(trny))*(1/(z-1))\n",
        "      self.a4=self.a4+self.lr*sum(dcrs*dz*np.array(trnx[atr[3]]))\n",
        "      logits=self.a1*trnx[atr[0]]+self.a2*trnx[atr[1]]+self.a3*trnx[atr[2]]+self.a4*trnx[atr[3]]+self.a\n",
        "      z=1/(1+np.exp(-np.array(logits)))+0.0000001\n",
        "      dz=z*(1-z)\n",
        "      dcrs=np.array(trny)*(1/z)+(1-np.array(trny))*(1/(z-1))\n",
        "      self.a=self.a+self.lr*sum(dcrs*dz)\n",
        "      loss2=self.Loss(trnx,trny)\n",
        "      dL=loss1-loss2\n",
        "      cnt=cnt+1\n",
        "      print('Iteration : {}     Loss : {}'.format(cnt,loss2))\n",
        "\n",
        "  # The acc method used to calculate accuracy \n",
        "  # It calculates both train as well as test accuracy\n",
        "  \n",
        "  def acc(self):\n",
        "    trnx=self.trnx\n",
        "    atr=list(trnx.columns)\n",
        "    logits=self.a1*trnx[atr[0]]+self.a2*trnx[atr[1]]+self.a3*trnx[atr[2]]+self.a4*trnx[atr[3]]+self.a\n",
        "    z=1/(1+np.exp(-np.array(logits)))\n",
        "    pr=[0.0 if i<0.5 else 1.0 for i in z]\n",
        "    pr=pd.Series(pr)\n",
        "    x=pr==self.trny\n",
        "    trn_acc=x.value_counts()[True]/len(pr)\n",
        "\n",
        "\n",
        "    trnx=self.tstx\n",
        "    atr=list(trnx.columns)\n",
        "    logits=self.a1*trnx[atr[0]]+self.a2*trnx[atr[1]]+self.a3*trnx[atr[2]]+self.a4*trnx[atr[3]]+self.a\n",
        "    z=1/(1+np.exp(-np.array(logits)))\n",
        "    pr=[0.0 if i<0.5 else 1.0 for i in z]\n",
        "    pr=pd.Series(pr)\n",
        "    x=pr==self.tsty\n",
        "    tst_acc=x.value_counts()[True]/len(pr)\n",
        "\n",
        "    print('Training accuracy = {} \\n Test Accuracy = {}'.format(trn_acc*100,tst_acc*100))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el0M2ZJYUXJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instanciating the class\n",
        "lg=Logistic_Regression(trn,tst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7DCtW7YUW_p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "24f62088-819e-40cb-b0aa-e203a749bcdb"
      },
      "source": [
        "lg.Train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration : 1     Loss : 547.1380052897094\n",
            "Iteration : 2     Loss : 547.0830682607914\n",
            "Iteration : 3     Loss : 547.027628903704\n",
            "Iteration : 4     Loss : 546.9716833331898\n",
            "Iteration : 5     Loss : 546.9152276409123\n",
            "Iteration : 6     Loss : 546.8582579002017\n",
            "Iteration : 7     Loss : 546.800770167486\n",
            "Iteration : 8     Loss : 546.7427604747209\n",
            "Iteration : 9     Loss : 546.6842248373234\n",
            "Iteration : 10     Loss : 546.625159249776\n",
            "Iteration : 11     Loss : 546.5655596862514\n",
            "Iteration : 12     Loss : 546.5054221032099\n",
            "Iteration : 13     Loss : 546.444742436647\n",
            "Iteration : 14     Loss : 546.3835166024264\n",
            "Iteration : 15     Loss : 546.3217404982946\n",
            "Iteration : 16     Loss : 546.2594099996834\n",
            "Iteration : 17     Loss : 546.1965209685445\n",
            "Iteration : 18     Loss : 546.1330692434274\n",
            "Iteration : 19     Loss : 546.0690506444272\n",
            "Iteration : 20     Loss : 546.0044609736343\n",
            "Iteration : 21     Loss : 545.9392960115678\n",
            "Iteration : 22     Loss : 545.8735515273399\n",
            "Iteration : 23     Loss : 545.8072232623274\n",
            "Iteration : 24     Loss : 545.7403069445853\n",
            "Iteration : 25     Loss : 545.6727982823605\n",
            "Iteration : 26     Loss : 545.6046929664005\n",
            "Iteration : 27     Loss : 545.5359866698374\n",
            "Iteration : 28     Loss : 545.4666750438975\n",
            "Iteration : 29     Loss : 545.3967537256649\n",
            "Iteration : 30     Loss : 545.3262183347407\n",
            "Iteration : 31     Loss : 545.2550644701181\n",
            "Iteration : 32     Loss : 545.1832877155812\n",
            "Iteration : 33     Loss : 545.1108836352886\n",
            "Iteration : 34     Loss : 545.0378477801568\n",
            "Iteration : 35     Loss : 544.9641756782196\n",
            "Iteration : 36     Loss : 544.8898628455026\n",
            "Iteration : 37     Loss : 544.8149047772721\n",
            "Iteration : 38     Loss : 544.7392969559079\n",
            "Iteration : 39     Loss : 544.6630348440672\n",
            "Iteration : 40     Loss : 544.58611388938\n",
            "Iteration : 41     Loss : 544.5085295250847\n",
            "Iteration : 42     Loss : 544.4302771622197\n",
            "Iteration : 43     Loss : 544.3513522044952\n",
            "Iteration : 44     Loss : 544.271750033033\n",
            "Iteration : 45     Loss : 544.1914660149969\n",
            "Iteration : 46     Loss : 544.110495505528\n",
            "Iteration : 47     Loss : 544.0288338412223\n",
            "Iteration : 48     Loss : 543.9464763417377\n",
            "Iteration : 49     Loss : 543.8634183173989\n",
            "Iteration : 50     Loss : 543.7796550600435\n",
            "Iteration : 51     Loss : 543.6951818488266\n",
            "Iteration : 52     Loss : 543.6099939462088\n",
            "Iteration : 53     Loss : 543.5240866043753\n",
            "Iteration : 54     Loss : 543.4374550596668\n",
            "Iteration : 55     Loss : 543.3500945314113\n",
            "Iteration : 56     Loss : 543.2620002316917\n",
            "Iteration : 57     Loss : 543.1731673557695\n",
            "Iteration : 58     Loss : 543.0835910875594\n",
            "Iteration : 59     Loss : 542.993266594858\n",
            "Iteration : 60     Loss : 542.9021890386213\n",
            "Iteration : 61     Loss : 542.8103535622406\n",
            "Iteration : 62     Loss : 542.7177552982411\n",
            "Iteration : 63     Loss : 542.6243893707787\n",
            "Iteration : 64     Loss : 542.530250886022\n",
            "Iteration : 65     Loss : 542.4353349455506\n",
            "Iteration : 66     Loss : 542.3396366359481\n",
            "Iteration : 67     Loss : 542.2431510330548\n",
            "Iteration : 68     Loss : 542.1458732022797\n",
            "Iteration : 69     Loss : 542.0477982009498\n",
            "Iteration : 70     Loss : 541.9489210740601\n",
            "Iteration : 71     Loss : 541.8492368554371\n",
            "Iteration : 72     Loss : 541.7487405738744\n",
            "Iteration : 73     Loss : 541.6474272441856\n",
            "Iteration : 74     Loss : 541.5452918753297\n",
            "Iteration : 75     Loss : 541.4423294650986\n",
            "Iteration : 76     Loss : 541.3385350051783\n",
            "Iteration : 77     Loss : 541.2339034779837\n",
            "Iteration : 78     Loss : 541.1284298575571\n",
            "Iteration : 79     Loss : 541.0221091103657\n",
            "Iteration : 80     Loss : 540.9149361972649\n",
            "Iteration : 81     Loss : 540.8069060698871\n",
            "Iteration : 82     Loss : 540.6980136764336\n",
            "Iteration : 83     Loss : 540.588253954551\n",
            "Iteration : 84     Loss : 540.4776218387714\n",
            "Iteration : 85     Loss : 540.3661122570414\n",
            "Iteration : 86     Loss : 540.2537201312001\n",
            "Iteration : 87     Loss : 540.140440379012\n",
            "Iteration : 88     Loss : 540.0262679146542\n",
            "Iteration : 89     Loss : 539.91119764451\n",
            "Iteration : 90     Loss : 539.7952244722942\n",
            "Iteration : 91     Loss : 539.6783433005353\n",
            "Iteration : 92     Loss : 539.560549025246\n",
            "Iteration : 93     Loss : 539.4418365399664\n",
            "Iteration : 94     Loss : 539.3222007368854\n",
            "Iteration : 95     Loss : 539.2016365041575\n",
            "Iteration : 96     Loss : 539.080138728785\n",
            "Iteration : 97     Loss : 538.9577022965593\n",
            "Iteration : 98     Loss : 538.8343220916744\n",
            "Iteration : 99     Loss : 538.7099929967621\n",
            "Iteration : 100     Loss : 538.5847098944804\n",
            "Iteration : 101     Loss : 538.4584676681229\n",
            "Iteration : 102     Loss : 538.3312611997328\n",
            "Iteration : 103     Loss : 538.2030853740688\n",
            "Iteration : 104     Loss : 538.0739350741753\n",
            "Iteration : 105     Loss : 537.9438051879873\n",
            "Iteration : 106     Loss : 537.8126906020141\n",
            "Iteration : 107     Loss : 537.6805862060721\n",
            "Iteration : 108     Loss : 537.5474868943506\n",
            "Iteration : 109     Loss : 537.4133875624694\n",
            "Iteration : 110     Loss : 537.2782831097926\n",
            "Iteration : 111     Loss : 537.1421684397907\n",
            "Iteration : 112     Loss : 537.0050384599359\n",
            "Iteration : 113     Loss : 536.86688808398\n",
            "Iteration : 114     Loss : 536.7277122288212\n",
            "Iteration : 115     Loss : 536.5875058193193\n",
            "Iteration : 116     Loss : 536.446263784263\n",
            "Iteration : 117     Loss : 536.3039810606975\n",
            "Iteration : 118     Loss : 536.1606525920403\n",
            "Iteration : 119     Loss : 536.0162733298738\n",
            "Iteration : 120     Loss : 535.8708382332527\n",
            "Iteration : 121     Loss : 535.7243422696507\n",
            "Iteration : 122     Loss : 535.5767804162293\n",
            "Iteration : 123     Loss : 535.428147658593\n",
            "Iteration : 124     Loss : 535.2784389915863\n",
            "Iteration : 125     Loss : 535.1276494255605\n",
            "Iteration : 126     Loss : 534.9757739754316\n",
            "Iteration : 127     Loss : 534.8228076702139\n",
            "Iteration : 128     Loss : 534.6687455511807\n",
            "Iteration : 129     Loss : 534.5135826741509\n",
            "Iteration : 130     Loss : 534.3573141028689\n",
            "Iteration : 131     Loss : 534.199934917589\n",
            "Iteration : 132     Loss : 534.0414402157141\n",
            "Iteration : 133     Loss : 533.8818251033034\n",
            "Iteration : 134     Loss : 533.7210847051316\n",
            "Iteration : 135     Loss : 533.5592141617835\n",
            "Iteration : 136     Loss : 533.396208628433\n",
            "Iteration : 137     Loss : 533.2320632810088\n",
            "Iteration : 138     Loss : 533.0667733064333\n",
            "Iteration : 139     Loss : 532.9003339151582\n",
            "Iteration : 140     Loss : 532.7327403340793\n",
            "Iteration : 141     Loss : 532.5639878109517\n",
            "Iteration : 142     Loss : 532.3940716088472\n",
            "Iteration : 143     Loss : 532.2229870172687\n",
            "Iteration : 144     Loss : 532.0507293421474\n",
            "Iteration : 145     Loss : 531.8772939128621\n",
            "Iteration : 146     Loss : 531.70267607993\n",
            "Iteration : 147     Loss : 531.5268712191049\n",
            "Iteration : 148     Loss : 531.3498747270079\n",
            "Iteration : 149     Loss : 531.1716820243539\n",
            "Iteration : 150     Loss : 530.9922885597351\n",
            "Iteration : 151     Loss : 530.8116898021076\n",
            "Iteration : 152     Loss : 530.629881250643\n",
            "Iteration : 153     Loss : 530.446858429511\n",
            "Iteration : 154     Loss : 530.2626168906796\n",
            "Iteration : 155     Loss : 530.0771522116005\n",
            "Iteration : 156     Loss : 529.8904600026799\n",
            "Iteration : 157     Loss : 529.7025359012046\n",
            "Iteration : 158     Loss : 529.513375572819\n",
            "Iteration : 159     Loss : 529.3229747158445\n",
            "Iteration : 160     Loss : 529.1313290620494\n",
            "Iteration : 161     Loss : 528.9384343708654\n",
            "Iteration : 162     Loss : 528.7442864369356\n",
            "Iteration : 163     Loss : 528.548881087541\n",
            "Iteration : 164     Loss : 528.3522141844626\n",
            "Iteration : 165     Loss : 528.1542816244855\n",
            "Iteration : 166     Loss : 527.9550793394919\n",
            "Iteration : 167     Loss : 527.7546032978112\n",
            "Iteration : 168     Loss : 527.5528495048899\n",
            "Iteration : 169     Loss : 527.3498140034534\n",
            "Iteration : 170     Loss : 527.1454928750802\n",
            "Iteration : 171     Loss : 526.9398822411487\n",
            "Iteration : 172     Loss : 526.7329782605945\n",
            "Iteration : 173     Loss : 526.5247771362855\n",
            "Iteration : 174     Loss : 526.3152751092375\n",
            "Iteration : 175     Loss : 526.1044684654764\n",
            "Iteration : 176     Loss : 525.8923535320077\n",
            "Iteration : 177     Loss : 525.6789266800085\n",
            "Iteration : 178     Loss : 525.4641843246762\n",
            "Iteration : 179     Loss : 525.2481229264104\n",
            "Iteration : 180     Loss : 525.0307389934082\n",
            "Iteration : 181     Loss : 524.812029076637\n",
            "Iteration : 182     Loss : 524.5919897780066\n",
            "Iteration : 183     Loss : 524.3706177460783\n",
            "Iteration : 184     Loss : 524.1479096786474\n",
            "Iteration : 185     Loss : 523.9238623219964\n",
            "Iteration : 186     Loss : 523.6984724753682\n",
            "Iteration : 187     Loss : 523.4717369859666\n",
            "Iteration : 188     Loss : 523.243652755798\n",
            "Iteration : 189     Loss : 523.0142167382687\n",
            "Iteration : 190     Loss : 522.7834259408577\n",
            "Iteration : 191     Loss : 522.5512774244758\n",
            "Iteration : 192     Loss : 522.317768304809\n",
            "Iteration : 193     Loss : 522.0828957550485\n",
            "Iteration : 194     Loss : 521.8466570029549\n",
            "Iteration : 195     Loss : 521.609049335073\n",
            "Iteration : 196     Loss : 521.3700700947496\n",
            "Iteration : 197     Loss : 521.1297166851847\n",
            "Iteration : 198     Loss : 520.88798656863\n",
            "Iteration : 199     Loss : 520.6448772665441\n",
            "Iteration : 200     Loss : 520.4003863629799\n",
            "Iteration : 201     Loss : 520.1545115033291\n",
            "Iteration : 202     Loss : 519.9072503950713\n",
            "Iteration : 203     Loss : 519.6586008091818\n",
            "Iteration : 204     Loss : 519.4085605801171\n",
            "Iteration : 205     Loss : 519.1571276065387\n",
            "Iteration : 206     Loss : 518.9042998545567\n",
            "Iteration : 207     Loss : 518.6500753545918\n",
            "Iteration : 208     Loss : 518.394452204022\n",
            "Iteration : 209     Loss : 518.1374285677505\n",
            "Iteration : 210     Loss : 517.8790026779252\n",
            "Iteration : 211     Loss : 517.6191728387762\n",
            "Iteration : 212     Loss : 517.3579374203366\n",
            "Iteration : 213     Loss : 517.0952948635239\n",
            "Iteration : 214     Loss : 516.8312436811274\n",
            "Iteration : 215     Loss : 516.5657824573019\n",
            "Iteration : 216     Loss : 516.2989098476794\n",
            "Iteration : 217     Loss : 516.0306245800476\n",
            "Iteration : 218     Loss : 515.7609254579355\n",
            "Iteration : 219     Loss : 515.4898113552388\n",
            "Iteration : 220     Loss : 515.2172812229385\n",
            "Iteration : 221     Loss : 514.9433340865621\n",
            "Iteration : 222     Loss : 514.6679690472995\n",
            "Iteration : 223     Loss : 514.3911852813928\n",
            "Iteration : 224     Loss : 514.1129820439677\n",
            "Iteration : 225     Loss : 513.8333586654388\n",
            "Iteration : 226     Loss : 513.5523145558343\n",
            "Iteration : 227     Loss : 513.2698492017815\n",
            "Iteration : 228     Loss : 512.985962170028\n",
            "Iteration : 229     Loss : 512.7006531067171\n",
            "Iteration : 230     Loss : 512.4139217352537\n",
            "Iteration : 231     Loss : 512.1257678637714\n",
            "Iteration : 232     Loss : 511.83619137700094\n",
            "Iteration : 233     Loss : 511.54519224221485\n",
            "Iteration : 234     Loss : 511.2527705084944\n",
            "Iteration : 235     Loss : 510.9589263071852\n",
            "Iteration : 236     Loss : 510.66365985004074\n",
            "Iteration : 237     Loss : 510.3669714333387\n",
            "Iteration : 238     Loss : 510.0688614346419\n",
            "Iteration : 239     Loss : 509.7693303168826\n",
            "Iteration : 240     Loss : 509.4683786241029\n",
            "Iteration : 241     Loss : 509.1660069850672\n",
            "Iteration : 242     Loss : 508.8622161135396\n",
            "Iteration : 243     Loss : 508.55700680695185\n",
            "Iteration : 244     Loss : 508.25037994585523\n",
            "Iteration : 245     Loss : 507.9423364987048\n",
            "Iteration : 246     Loss : 507.63287751556106\n",
            "Iteration : 247     Loss : 507.3220041333826\n",
            "Iteration : 248     Loss : 507.0097175741753\n",
            "Iteration : 249     Loss : 506.6960191445689\n",
            "Iteration : 250     Loss : 506.380910238156\n",
            "Iteration : 251     Loss : 506.06439233211006\n",
            "Iteration : 252     Loss : 505.7464669906997\n",
            "Iteration : 253     Loss : 505.4271358624786\n",
            "Iteration : 254     Loss : 505.1064006833502\n",
            "Iteration : 255     Loss : 504.78426327348876\n",
            "Iteration : 256     Loss : 504.4607255385148\n",
            "Iteration : 257     Loss : 504.13578947094095\n",
            "Iteration : 258     Loss : 503.809457147001\n",
            "Iteration : 259     Loss : 503.4817307295921\n",
            "Iteration : 260     Loss : 503.15261246634765\n",
            "Iteration : 261     Loss : 502.8221046900937\n",
            "Iteration : 262     Loss : 502.49020981870484\n",
            "Iteration : 263     Loss : 502.1569303550071\n",
            "Iteration : 264     Loss : 501.8222688850694\n",
            "Iteration : 265     Loss : 501.4862280809172\n",
            "Iteration : 266     Loss : 501.14881069706377\n",
            "Iteration : 267     Loss : 500.8100195729986\n",
            "Iteration : 268     Loss : 500.469857631315\n",
            "Iteration : 269     Loss : 500.128327877486\n",
            "Iteration : 270     Loss : 499.7854333994908\n",
            "Iteration : 271     Loss : 499.441177368756\n",
            "Iteration : 272     Loss : 499.09556303708746\n",
            "Iteration : 273     Loss : 498.74859373904917\n",
            "Iteration : 274     Loss : 498.4002728896934\n",
            "Iteration : 275     Loss : 498.05060398535136\n",
            "Iteration : 276     Loss : 497.699590601935\n",
            "Iteration : 277     Loss : 497.3472363947641\n",
            "Iteration : 278     Loss : 496.9935450991524\n",
            "Iteration : 279     Loss : 496.6385205280201\n",
            "Iteration : 280     Loss : 496.28216657292455\n",
            "Iteration : 281     Loss : 495.9244872014501\n",
            "Iteration : 282     Loss : 495.5654864598425\n",
            "Iteration : 283     Loss : 495.20516846926614\n",
            "Iteration : 284     Loss : 494.8435374260146\n",
            "Iteration : 285     Loss : 494.48059760160174\n",
            "Iteration : 286     Loss : 494.11635334180943\n",
            "Iteration : 287     Loss : 493.7508090641501\n",
            "Iteration : 288     Loss : 493.3839692609989\n",
            "Iteration : 289     Loss : 493.0158384939386\n",
            "Iteration : 290     Loss : 492.6464213973014\n",
            "Iteration : 291     Loss : 492.27572267487494\n",
            "Iteration : 292     Loss : 491.90374709980887\n",
            "Iteration : 293     Loss : 491.5304995128886\n",
            "Iteration : 294     Loss : 491.1559848238754\n",
            "Iteration : 295     Loss : 490.7802080081301\n",
            "Iteration : 296     Loss : 490.4031741071332\n",
            "Iteration : 297     Loss : 490.0248882265408\n",
            "Iteration : 298     Loss : 489.64535553727313\n",
            "Iteration : 299     Loss : 489.26458127162397\n",
            "Iteration : 300     Loss : 488.8825707250232\n",
            "Iteration : 301     Loss : 488.49932925334286\n",
            "Iteration : 302     Loss : 488.1148622720104\n",
            "Iteration : 303     Loss : 487.72917525602145\n",
            "Iteration : 304     Loss : 487.3422737376414\n",
            "Iteration : 305     Loss : 486.95416330723344\n",
            "Iteration : 306     Loss : 486.5648496086983\n",
            "Iteration : 307     Loss : 486.17433834335225\n",
            "Iteration : 308     Loss : 485.78263526374525\n",
            "Iteration : 309     Loss : 485.38974617590986\n",
            "Iteration : 310     Loss : 484.99567693740585\n",
            "Iteration : 311     Loss : 484.60043345628344\n",
            "Iteration : 312     Loss : 484.20402168939785\n",
            "Iteration : 313     Loss : 483.8064476410324\n",
            "Iteration : 314     Loss : 483.40771736333545\n",
            "Iteration : 315     Loss : 483.0078369537411\n",
            "Iteration : 316     Loss : 482.6068125533772\n",
            "Iteration : 317     Loss : 482.2046503479769\n",
            "Iteration : 318     Loss : 481.8013565654724\n",
            "Iteration : 319     Loss : 481.3969374731779\n",
            "Iteration : 320     Loss : 480.9913993801018\n",
            "Iteration : 321     Loss : 480.5847486322215\n",
            "Iteration : 322     Loss : 480.1769916141238\n",
            "Iteration : 323     Loss : 479.7681347456732\n",
            "Iteration : 324     Loss : 479.358184482367\n",
            "Iteration : 325     Loss : 478.94714731268647\n",
            "Iteration : 326     Loss : 478.53502975826\n",
            "Iteration : 327     Loss : 478.1218383716824\n",
            "Iteration : 328     Loss : 477.7075797358879\n",
            "Iteration : 329     Loss : 477.29226046208174\n",
            "Iteration : 330     Loss : 476.8758871901321\n",
            "Iteration : 331     Loss : 476.4584665849151\n",
            "Iteration : 332     Loss : 476.04000533738764\n",
            "Iteration : 333     Loss : 475.62051016230714\n",
            "Iteration : 334     Loss : 475.1999877973054\n",
            "Iteration : 335     Loss : 474.77844500101804\n",
            "Iteration : 336     Loss : 474.3558885527163\n",
            "Iteration : 337     Loss : 473.9323252508849\n",
            "Iteration : 338     Loss : 473.50776191173594\n",
            "Iteration : 339     Loss : 473.0822053679302\n",
            "Iteration : 340     Loss : 472.6556624679304\n",
            "Iteration : 341     Loss : 472.2281400743822\n",
            "Iteration : 342     Loss : 471.7996450629698\n",
            "Iteration : 343     Loss : 471.3701843217216\n",
            "Iteration : 344     Loss : 470.93976474838513\n",
            "Iteration : 345     Loss : 470.5083932511229\n",
            "Iteration : 346     Loss : 470.0760767467015\n",
            "Iteration : 347     Loss : 469.6428221585013\n",
            "Iteration : 348     Loss : 469.2086364162869\n",
            "Iteration : 349     Loss : 468.7735264544343\n",
            "Iteration : 350     Loss : 468.3374992127142\n",
            "Iteration : 351     Loss : 467.9005616316689\n",
            "Iteration : 352     Loss : 467.4627206546005\n",
            "Iteration : 353     Loss : 467.02398322502944\n",
            "Iteration : 354     Loss : 466.5843562859279\n",
            "Iteration : 355     Loss : 466.1438467793914\n",
            "Iteration : 356     Loss : 465.70246164379824\n",
            "Iteration : 357     Loss : 465.26020781445385\n",
            "Iteration : 358     Loss : 464.8170922217018\n",
            "Iteration : 359     Loss : 464.37312179033773\n",
            "Iteration : 360     Loss : 463.92830343820054\n",
            "Iteration : 361     Loss : 463.48264407577904\n",
            "Iteration : 362     Loss : 463.0361506039399\n",
            "Iteration : 363     Loss : 462.58882991486644\n",
            "Iteration : 364     Loss : 462.140688889327\n",
            "Iteration : 365     Loss : 461.6917343968245\n",
            "Iteration : 366     Loss : 461.24197329459827\n",
            "Iteration : 367     Loss : 460.79141242558904\n",
            "Iteration : 368     Loss : 460.3400586191884\n",
            "Iteration : 369     Loss : 459.88791868917394\n",
            "Iteration : 370     Loss : 459.43499943359103\n",
            "Iteration : 371     Loss : 458.9813076329837\n",
            "Iteration : 372     Loss : 458.5268500504725\n",
            "Iteration : 373     Loss : 458.07163343057545\n",
            "Iteration : 374     Loss : 457.6156644985593\n",
            "Iteration : 375     Loss : 457.158949958998\n",
            "Iteration : 376     Loss : 456.70149649569873\n",
            "Iteration : 377     Loss : 456.24331077121866\n",
            "Iteration : 378     Loss : 455.7843994248449\n",
            "Iteration : 379     Loss : 455.324769073304\n",
            "Iteration : 380     Loss : 454.86442630915303\n",
            "Iteration : 381     Loss : 454.403377700237\n",
            "Iteration : 382     Loss : 453.94162978946804\n",
            "Iteration : 383     Loss : 453.47918909340603\n",
            "Iteration : 384     Loss : 453.0160621024186\n",
            "Iteration : 385     Loss : 452.5522552793493\n",
            "Iteration : 386     Loss : 452.08777505935103\n",
            "Iteration : 387     Loss : 451.6226278493318\n",
            "Iteration : 388     Loss : 451.1568200269378\n",
            "Iteration : 389     Loss : 450.690357940291\n",
            "Iteration : 390     Loss : 450.2232479075414\n",
            "Iteration : 391     Loss : 449.7554962161473\n",
            "Iteration : 392     Loss : 449.2871091224544\n",
            "Iteration : 393     Loss : 448.81809285101633\n",
            "Iteration : 394     Loss : 448.3484535944498\n",
            "Iteration : 395     Loss : 447.87819751262845\n",
            "Iteration : 396     Loss : 447.4073307324326\n",
            "Iteration : 397     Loss : 446.9358593473723\n",
            "Iteration : 398     Loss : 446.46378941677153\n",
            "Iteration : 399     Loss : 445.9911269659536\n",
            "Iteration : 400     Loss : 445.51787798531996\n",
            "Iteration : 401     Loss : 445.0440484303421\n",
            "Iteration : 402     Loss : 444.5696442208916\n",
            "Iteration : 403     Loss : 444.09467124159835\n",
            "Iteration : 404     Loss : 443.619135340026\n",
            "Iteration : 405     Loss : 443.14304232826083\n",
            "Iteration : 406     Loss : 442.6663979811601\n",
            "Iteration : 407     Loss : 442.18920803677446\n",
            "Iteration : 408     Loss : 441.7114781958507\n",
            "Iteration : 409     Loss : 441.2332141215861\n",
            "Iteration : 410     Loss : 440.7544214393552\n",
            "Iteration : 411     Loss : 440.275105736545\n",
            "Iteration : 412     Loss : 439.79527256239925\n",
            "Iteration : 413     Loss : 439.3149274278149\n",
            "Iteration : 414     Loss : 438.83407580477405\n",
            "Iteration : 415     Loss : 438.35272312682656\n",
            "Iteration : 416     Loss : 437.8708747883677\n",
            "Iteration : 417     Loss : 437.3885361446533\n",
            "Iteration : 418     Loss : 436.9057125118448\n",
            "Iteration : 419     Loss : 436.42240916658926\n",
            "Iteration : 420     Loss : 435.9386313461363\n",
            "Iteration : 421     Loss : 435.454384248132\n",
            "Iteration : 422     Loss : 434.96967303049666\n",
            "Iteration : 423     Loss : 434.4845028115166\n",
            "Iteration : 424     Loss : 433.998878669439\n",
            "Iteration : 425     Loss : 433.5128056427581\n",
            "Iteration : 426     Loss : 433.0262887299739\n",
            "Iteration : 427     Loss : 432.5393328896768\n",
            "Iteration : 428     Loss : 432.05194304037235\n",
            "Iteration : 429     Loss : 431.564124060544\n",
            "Iteration : 430     Loss : 431.07588078873863\n",
            "Iteration : 431     Loss : 430.5872180232555\n",
            "Iteration : 432     Loss : 430.0981405226533\n",
            "Iteration : 433     Loss : 429.60865300546806\n",
            "Iteration : 434     Loss : 429.1187601499065\n",
            "Iteration : 435     Loss : 428.62846659475065\n",
            "Iteration : 436     Loss : 428.1377769384671\n",
            "Iteration : 437     Loss : 427.6466957399828\n",
            "Iteration : 438     Loss : 427.15522751835823\n",
            "Iteration : 439     Loss : 426.6633767529599\n",
            "Iteration : 440     Loss : 426.17114788358845\n",
            "Iteration : 441     Loss : 425.6785453103896\n",
            "Iteration : 442     Loss : 425.18557339431646\n",
            "Iteration : 443     Loss : 424.69223645667785\n",
            "Iteration : 444     Loss : 424.1985387798825\n",
            "Iteration : 445     Loss : 423.7044846071247\n",
            "Iteration : 446     Loss : 423.2100781424252\n",
            "Iteration : 447     Loss : 422.71532355126635\n",
            "Iteration : 448     Loss : 422.22022496015444\n",
            "Iteration : 449     Loss : 421.72478645712505\n",
            "Iteration : 450     Loss : 421.22901209187523\n",
            "Iteration : 451     Loss : 420.7329058756619\n",
            "Iteration : 452     Loss : 420.23647178171353\n",
            "Iteration : 453     Loss : 419.7397137452713\n",
            "Iteration : 454     Loss : 419.242635663741\n",
            "Iteration : 455     Loss : 418.7452413969896\n",
            "Iteration : 456     Loss : 418.2475347674316\n",
            "Iteration : 457     Loss : 417.7495195602147\n",
            "Iteration : 458     Loss : 417.25119952348774\n",
            "Iteration : 459     Loss : 416.7525783682477\n",
            "Iteration : 460     Loss : 416.2536597692857\n",
            "Iteration : 461     Loss : 415.75444736439556\n",
            "Iteration : 462     Loss : 415.25494475542274\n",
            "Iteration : 463     Loss : 414.75515550806244\n",
            "Iteration : 464     Loss : 414.2550831521538\n",
            "Iteration : 465     Loss : 413.75473118176427\n",
            "Iteration : 466     Loss : 413.2541030557294\n",
            "Iteration : 467     Loss : 412.7532021974886\n",
            "Iteration : 468     Loss : 412.2520319956317\n",
            "Iteration : 469     Loss : 411.7505958039236\n",
            "Iteration : 470     Loss : 411.2488969415676\n",
            "Iteration : 471     Loss : 410.746938693591\n",
            "Iteration : 472     Loss : 410.244724310684\n",
            "Iteration : 473     Loss : 409.742257010022\n",
            "Iteration : 474     Loss : 409.23953997489764\n",
            "Iteration : 475     Loss : 408.736576355466\n",
            "Iteration : 476     Loss : 408.2333692685932\n",
            "Iteration : 477     Loss : 407.7299217983484\n",
            "Iteration : 478     Loss : 407.22623699629145\n",
            "Iteration : 479     Loss : 406.72231788128664\n",
            "Iteration : 480     Loss : 406.21816744032554\n",
            "Iteration : 481     Loss : 405.71378862839595\n",
            "Iteration : 482     Loss : 405.2091843689201\n",
            "Iteration : 483     Loss : 404.7043575539402\n",
            "Iteration : 484     Loss : 404.19931104421096\n",
            "Iteration : 485     Loss : 403.69404766981677\n",
            "Iteration : 486     Loss : 403.18857023000794\n",
            "Iteration : 487     Loss : 402.68288149387297\n",
            "Iteration : 488     Loss : 402.1769842001771\n",
            "Iteration : 489     Loss : 401.6708810580157\n",
            "Iteration : 490     Loss : 401.16457474668164\n",
            "Iteration : 491     Loss : 400.6580679163089\n",
            "Iteration : 492     Loss : 400.1513631877972\n",
            "Iteration : 493     Loss : 399.64446315333515\n",
            "Iteration : 494     Loss : 399.1373703764129\n",
            "Iteration : 495     Loss : 398.63008739228957\n",
            "Iteration : 496     Loss : 398.12261670810267\n",
            "Iteration : 497     Loss : 397.6149608032908\n",
            "Iteration : 498     Loss : 397.10712212955906\n",
            "Iteration : 499     Loss : 396.59910311153635\n",
            "Iteration : 500     Loss : 396.09090614665314\n",
            "Iteration : 501     Loss : 395.58253360565743\n",
            "Iteration : 502     Loss : 395.0739878327339\n",
            "Iteration : 503     Loss : 394.56527114583713\n",
            "Iteration : 504     Loss : 394.0563858368401\n",
            "Iteration : 505     Loss : 393.5473341719402\n",
            "Iteration : 506     Loss : 393.03811839184226\n",
            "Iteration : 507     Loss : 392.52874071189456\n",
            "Iteration : 508     Loss : 392.019203322546\n",
            "Iteration : 509     Loss : 391.5095083894343\n",
            "Iteration : 510     Loss : 390.9996580538418\n",
            "Iteration : 511     Loss : 390.4896544326569\n",
            "Iteration : 512     Loss : 389.97949961877333\n",
            "Iteration : 513     Loss : 389.46919568149093\n",
            "Iteration : 514     Loss : 388.9587446664581\n",
            "Iteration : 515     Loss : 388.4481485961414\n",
            "Iteration : 516     Loss : 387.93740946996303\n",
            "Iteration : 517     Loss : 387.42652926462614\n",
            "Iteration : 518     Loss : 386.9155099343004\n",
            "Iteration : 519     Loss : 386.40435341091523\n",
            "Iteration : 520     Loss : 385.893061604221\n",
            "Iteration : 521     Loss : 385.3816364023538\n",
            "Iteration : 522     Loss : 384.8700796717579\n",
            "Iteration : 523     Loss : 384.35839325765954\n",
            "Iteration : 524     Loss : 383.84657898412866\n",
            "Iteration : 525     Loss : 383.33463865438773\n",
            "Iteration : 526     Loss : 382.82257405106225\n",
            "Iteration : 527     Loss : 382.31038693639664\n",
            "Iteration : 528     Loss : 381.79807905244667\n",
            "Iteration : 529     Loss : 381.2856521213995\n",
            "Iteration : 530     Loss : 380.7731078456866\n",
            "Iteration : 531     Loss : 380.2604479082835\n",
            "Iteration : 532     Loss : 379.74767397290213\n",
            "Iteration : 533     Loss : 379.23478768432466\n",
            "Iteration : 534     Loss : 378.72179066838345\n",
            "Iteration : 535     Loss : 378.2086845324601\n",
            "Iteration : 536     Loss : 377.69547086545975\n",
            "Iteration : 537     Loss : 377.18215123822824\n",
            "Iteration : 538     Loss : 376.6687272036675\n",
            "Iteration : 539     Loss : 376.1552002969364\n",
            "Iteration : 540     Loss : 375.6415720357136\n",
            "Iteration : 541     Loss : 375.1278439203472\n",
            "Iteration : 542     Loss : 374.6140174341721\n",
            "Iteration : 543     Loss : 374.1000940435901\n",
            "Iteration : 544     Loss : 373.58607519840024\n",
            "Iteration : 545     Loss : 373.07196233186744\n",
            "Iteration : 546     Loss : 372.55775686106256\n",
            "Iteration : 547     Loss : 372.04346018695645\n",
            "Iteration : 548     Loss : 371.52907369471654\n",
            "Iteration : 549     Loss : 371.0145987537818\n",
            "Iteration : 550     Loss : 370.5000367182145\n",
            "Iteration : 551     Loss : 369.98538892676964\n",
            "Iteration : 552     Loss : 369.47065670313066\n",
            "Iteration : 553     Loss : 368.9558413561396\n",
            "Iteration : 554     Loss : 368.44094417994046\n",
            "Iteration : 555     Loss : 367.92596645414704\n",
            "Iteration : 556     Loss : 367.4109094441221\n",
            "Iteration : 557     Loss : 366.8957744010973\n",
            "Iteration : 558     Loss : 366.38056256233773\n",
            "Iteration : 559     Loss : 365.86527515139045\n",
            "Iteration : 560     Loss : 365.34991337822777\n",
            "Iteration : 561     Loss : 364.83447843942145\n",
            "Iteration : 562     Loss : 364.3189715183508\n",
            "Iteration : 563     Loss : 363.80339378534734\n",
            "Iteration : 564     Loss : 363.28774639789003\n",
            "Iteration : 565     Loss : 362.7720305007896\n",
            "Iteration : 566     Loss : 362.2562472263357\n",
            "Iteration : 567     Loss : 361.7403976944592\n",
            "Iteration : 568     Loss : 361.22448301294656\n",
            "Iteration : 569     Loss : 360.7085042775907\n",
            "Iteration : 570     Loss : 360.1924625723313\n",
            "Iteration : 571     Loss : 359.6763589694593\n",
            "Iteration : 572     Loss : 359.16019452972944\n",
            "Iteration : 573     Loss : 358.64397030257925\n",
            "Iteration : 574     Loss : 358.12768732624625\n",
            "Iteration : 575     Loss : 357.61134662799816\n",
            "Iteration : 576     Loss : 357.09494922416246\n",
            "Iteration : 577     Loss : 356.5784961203963\n",
            "Iteration : 578     Loss : 356.06198831179995\n",
            "Iteration : 579     Loss : 355.5454267831028\n",
            "Iteration : 580     Loss : 355.02881250873077\n",
            "Iteration : 581     Loss : 354.5121464530525\n",
            "Iteration : 582     Loss : 353.99542957049005\n",
            "Iteration : 583     Loss : 353.4786628056662\n",
            "Iteration : 584     Loss : 352.96184709353395\n",
            "Iteration : 585     Loss : 352.44498335956763\n",
            "Iteration : 586     Loss : 351.9280725198633\n",
            "Iteration : 587     Loss : 351.41111548130107\n",
            "Iteration : 588     Loss : 350.8941131416975\n",
            "Iteration : 589     Loss : 350.37706638990534\n",
            "Iteration : 590     Loss : 349.8599761060164\n",
            "Iteration : 591     Loss : 349.3428431614254\n",
            "Iteration : 592     Loss : 348.8256684190454\n",
            "Iteration : 593     Loss : 348.308452733361\n",
            "Iteration : 594     Loss : 347.7911969506323\n",
            "Iteration : 595     Loss : 347.27390190898404\n",
            "Iteration : 596     Loss : 346.75656843855876\n",
            "Iteration : 597     Loss : 346.2391973616338\n",
            "Iteration : 598     Loss : 345.72178949275593\n",
            "Iteration : 599     Loss : 345.2043456388853\n",
            "Iteration : 600     Loss : 344.6868665994767\n",
            "Iteration : 601     Loss : 344.1693531666963\n",
            "Iteration : 602     Loss : 343.6518061254037\n",
            "Iteration : 603     Loss : 343.13422625341394\n",
            "Iteration : 604     Loss : 342.6166143215395\n",
            "Iteration : 605     Loss : 342.0989710937579\n",
            "Iteration : 606     Loss : 341.5812973272866\n",
            "Iteration : 607     Loss : 341.06359377273253\n",
            "Iteration : 608     Loss : 340.54586117420416\n",
            "Iteration : 609     Loss : 340.0281002694243\n",
            "Iteration : 610     Loss : 339.5103117898422\n",
            "Iteration : 611     Loss : 338.99249646076953\n",
            "Iteration : 612     Loss : 338.4746550014633\n",
            "Iteration : 613     Loss : 337.9567881252769\n",
            "Iteration : 614     Loss : 337.4388965397156\n",
            "Iteration : 615     Loss : 336.9209809466043\n",
            "Iteration : 616     Loss : 336.4030420421742\n",
            "Iteration : 617     Loss : 335.8850805171701\n",
            "Iteration : 618     Loss : 335.36709705695523\n",
            "Iteration : 619     Loss : 334.8490923416334\n",
            "Iteration : 620     Loss : 334.33106704612896\n",
            "Iteration : 621     Loss : 333.81302184032984\n",
            "Iteration : 622     Loss : 333.2949573891568\n",
            "Iteration : 623     Loss : 332.7768743526876\n",
            "Iteration : 624     Loss : 332.25877338623616\n",
            "Iteration : 625     Loss : 331.7406551405154\n",
            "Iteration : 626     Loss : 331.22252026163585\n",
            "Iteration : 627     Loss : 330.7043693913088\n",
            "Iteration : 628     Loss : 330.1862031668772\n",
            "Iteration : 629     Loss : 329.6680222214479\n",
            "Iteration : 630     Loss : 329.14982718398215\n",
            "Iteration : 631     Loss : 328.63161867938766\n",
            "Iteration : 632     Loss : 328.11339732861103\n",
            "Iteration : 633     Loss : 327.59516374874033\n",
            "Iteration : 634     Loss : 327.07691855311555\n",
            "Iteration : 635     Loss : 326.55866235138444\n",
            "Iteration : 636     Loss : 326.04039574962957\n",
            "Iteration : 637     Loss : 325.5221193504473\n",
            "Iteration : 638     Loss : 325.00383375303494\n",
            "Iteration : 639     Loss : 324.48553955330243\n",
            "Iteration : 640     Loss : 323.96723734393544\n",
            "Iteration : 641     Loss : 323.448927714508\n",
            "Iteration : 642     Loss : 322.9306112515512\n",
            "Iteration : 643     Loss : 322.412288538666\n",
            "Iteration : 644     Loss : 321.89396015658923\n",
            "Iteration : 645     Loss : 321.37562668329997\n",
            "Iteration : 646     Loss : 320.8572886940814\n",
            "Iteration : 647     Loss : 320.33894676163277\n",
            "Iteration : 648     Loss : 319.8206014561462\n",
            "Iteration : 649     Loss : 319.30225334538517\n",
            "Iteration : 650     Loss : 318.78390299477314\n",
            "Iteration : 651     Loss : 318.2655509674814\n",
            "Iteration : 652     Loss : 317.74719782450666\n",
            "Iteration : 653     Loss : 317.22884412476543\n",
            "Iteration : 654     Loss : 316.71049042515045\n",
            "Iteration : 655     Loss : 316.19213728064756\n",
            "Iteration : 656     Loss : 315.67378524439005\n",
            "Iteration : 657     Loss : 315.1554348677507\n",
            "Iteration : 658     Loss : 314.63708670042456\n",
            "Iteration : 659     Loss : 314.11874129049784\n",
            "Iteration : 660     Loss : 313.60039918454345\n",
            "Iteration : 661     Loss : 313.0820609276815\n",
            "Iteration : 662     Loss : 312.56372706368086\n",
            "Iteration : 663     Loss : 312.0453981350149\n",
            "Iteration : 664     Loss : 311.5270746829543\n",
            "Iteration : 665     Loss : 311.0087572476377\n",
            "Iteration : 666     Loss : 310.49044636814307\n",
            "Iteration : 667     Loss : 309.9721425825966\n",
            "Iteration : 668     Loss : 309.45384642819386\n",
            "Iteration : 669     Loss : 308.93555844133033\n",
            "Iteration : 670     Loss : 308.41727915764136\n",
            "Iteration : 671     Loss : 307.899009112097\n",
            "Iteration : 672     Loss : 307.3807488390649\n",
            "Iteration : 673     Loss : 306.8624988723916\n",
            "Iteration : 674     Loss : 306.3442597454804\n",
            "Iteration : 675     Loss : 305.82603199135497\n",
            "Iteration : 676     Loss : 305.30781614274264\n",
            "Iteration : 677     Loss : 304.7896127321457\n",
            "Iteration : 678     Loss : 304.27142229191253\n",
            "Iteration : 679     Loss : 303.75324535431\n",
            "Iteration : 680     Loss : 303.2350824516013\n",
            "Iteration : 681     Loss : 302.7169341161127\n",
            "Iteration : 682     Loss : 302.1988008803179\n",
            "Iteration : 683     Loss : 301.6806832768933\n",
            "Iteration : 684     Loss : 301.1625818387966\n",
            "Iteration : 685     Loss : 300.64449709935303\n",
            "Iteration : 686     Loss : 300.1264295923019\n",
            "Iteration : 687     Loss : 299.608379851884\n",
            "Iteration : 688     Loss : 299.0903484129139\n",
            "Iteration : 689     Loss : 298.57233581084125\n",
            "Iteration : 690     Loss : 298.0543425818369\n",
            "Iteration : 691     Loss : 297.53636926284224\n",
            "Iteration : 692     Loss : 297.0184163916635\n",
            "Iteration : 693     Loss : 296.5004845070265\n",
            "Iteration : 694     Loss : 295.982574148646\n",
            "Iteration : 695     Loss : 295.46468585731685\n",
            "Iteration : 696     Loss : 294.94682017495785\n",
            "Iteration : 697     Loss : 294.4289776446953\n",
            "Iteration : 698     Loss : 293.9111588109409\n",
            "Iteration : 699     Loss : 293.39336421944256\n",
            "Iteration : 700     Loss : 292.87559441736704\n",
            "Iteration : 701     Loss : 292.357849953374\n",
            "Iteration : 702     Loss : 291.8401313776727\n",
            "Iteration : 703     Loss : 291.3224392421047\n",
            "Iteration : 704     Loss : 290.80477410020603\n",
            "Iteration : 705     Loss : 290.2871365072799\n",
            "Iteration : 706     Loss : 289.769527020466\n",
            "Iteration : 707     Loss : 289.2519461988137\n",
            "Iteration : 708     Loss : 288.73439460334515\n",
            "Iteration : 709     Loss : 288.2168727971313\n",
            "Iteration : 710     Loss : 287.69938134536085\n",
            "Iteration : 711     Loss : 287.1819208154099\n",
            "Iteration : 712     Loss : 286.66449177691\n",
            "Iteration : 713     Loss : 286.14709480182114\n",
            "Iteration : 714     Loss : 285.6297304644998\n",
            "Iteration : 715     Loss : 285.11239934177496\n",
            "Iteration : 716     Loss : 284.59510201300606\n",
            "Iteration : 717     Loss : 284.07783906017136\n",
            "Iteration : 718     Loss : 283.56061106792157\n",
            "Iteration : 719     Loss : 283.0434186236594\n",
            "Iteration : 720     Loss : 282.52626231760917\n",
            "Iteration : 721     Loss : 282.0091427428871\n",
            "Iteration : 722     Loss : 281.49206049557426\n",
            "Iteration : 723     Loss : 280.97501617478326\n",
            "Iteration : 724     Loss : 280.4580103827341\n",
            "Iteration : 725     Loss : 279.9410437248241\n",
            "Iteration : 726     Loss : 279.4241168096997\n",
            "Iteration : 727     Loss : 278.907230249326\n",
            "Iteration : 728     Loss : 278.39038465906424\n",
            "Iteration : 729     Loss : 277.8735806577395\n",
            "Iteration : 730     Loss : 277.35681886771147\n",
            "Iteration : 731     Loss : 276.8400999149546\n",
            "Iteration : 732     Loss : 276.3234244291226\n",
            "Iteration : 733     Loss : 275.8067930436273\n",
            "Iteration : 734     Loss : 275.2902063957067\n",
            "Iteration : 735     Loss : 274.7736651265063\n",
            "Iteration : 736     Loss : 274.2571698811422\n",
            "Iteration : 737     Loss : 273.74072130878307\n",
            "Iteration : 738     Loss : 273.22432006272544\n",
            "Iteration : 739     Loss : 272.7079668004599\n",
            "Iteration : 740     Loss : 272.1916621837505\n",
            "Iteration : 741     Loss : 271.6754068787168\n",
            "Iteration : 742     Loss : 271.15920155589396\n",
            "Iteration : 743     Loss : 270.64304689032093\n",
            "Iteration : 744     Loss : 270.1269435616114\n",
            "Iteration : 745     Loss : 269.61089225403214\n",
            "Iteration : 746     Loss : 269.094893656577\n",
            "Iteration : 747     Loss : 268.57894846304225\n",
            "Iteration : 748     Loss : 268.06305737211113\n",
            "Iteration : 749     Loss : 267.54722108742163\n",
            "Iteration : 750     Loss : 267.031440317653\n",
            "Iteration : 751     Loss : 266.51571577659877\n",
            "Iteration : 752     Loss : 266.00004818324453\n",
            "Iteration : 753     Loss : 265.4844382618529\n",
            "Iteration : 754     Loss : 264.96888674203365\n",
            "Iteration : 755     Loss : 264.45339435883443\n",
            "Iteration : 756     Loss : 263.93796185281053\n",
            "Iteration : 757     Loss : 263.42258997010947\n",
            "Iteration : 758     Loss : 262.9072794625528\n",
            "Iteration : 759     Loss : 262.3920310877176\n",
            "Iteration : 760     Loss : 261.8768456090146\n",
            "Iteration : 761     Loss : 261.3617237957728\n",
            "Iteration : 762     Loss : 260.8466664233226\n",
            "Iteration : 763     Loss : 260.33167427307563\n",
            "Iteration : 764     Loss : 259.81674813261355\n",
            "Iteration : 765     Loss : 259.30188879576394\n",
            "Iteration : 766     Loss : 258.78709706268927\n",
            "Iteration : 767     Loss : 258.2723737399752\n",
            "Iteration : 768     Loss : 257.75771964070674\n",
            "Iteration : 769     Loss : 257.24313558456083\n",
            "Iteration : 770     Loss : 256.7286223978895\n",
            "Iteration : 771     Loss : 256.21418091380616\n",
            "Iteration : 772     Loss : 255.69981197227577\n",
            "Iteration : 773     Loss : 255.18551642019818\n",
            "Iteration : 774     Loss : 254.6712951114975\n",
            "Iteration : 775     Loss : 254.15714890721415\n",
            "Iteration : 776     Loss : 253.6430786755864\n",
            "Iteration : 777     Loss : 253.12908529214627\n",
            "Iteration : 778     Loss : 252.61516963980844\n",
            "Iteration : 779     Loss : 252.10133260895944\n",
            "Iteration : 780     Loss : 251.58757509754844\n",
            "Iteration : 781     Loss : 251.07389801117978\n",
            "Iteration : 782     Loss : 250.56030226320655\n",
            "Iteration : 783     Loss : 250.04678877481913\n",
            "Iteration : 784     Loss : 249.5333584751438\n",
            "Iteration : 785     Loss : 249.02001230133232\n",
            "Iteration : 786     Loss : 248.5067511986586\n",
            "Iteration : 787     Loss : 247.99357612061294\n",
            "Iteration : 788     Loss : 247.4804880289976\n",
            "Iteration : 789     Loss : 246.96748789402287\n",
            "Iteration : 790     Loss : 246.45457669440373\n",
            "Iteration : 791     Loss : 245.9417554174583\n",
            "Iteration : 792     Loss : 245.42902505920316\n",
            "Iteration : 793     Loss : 244.91638662445604\n",
            "Iteration : 794     Loss : 244.4038411269285\n",
            "Iteration : 795     Loss : 243.89138958933358\n",
            "Iteration : 796     Loss : 243.37903304347907\n",
            "Iteration : 797     Loss : 242.86677253037254\n",
            "Iteration : 798     Loss : 242.3546091003216\n",
            "Iteration : 799     Loss : 241.84254381303714\n",
            "Iteration : 800     Loss : 241.33057773773217\n",
            "Iteration : 801     Loss : 240.8187119532327\n",
            "Iteration : 802     Loss : 240.3069475480747\n",
            "Iteration : 803     Loss : 239.7952856206116\n",
            "Iteration : 804     Loss : 239.2837272791204\n",
            "Iteration : 805     Loss : 238.77227364190665\n",
            "Iteration : 806     Loss : 238.26092583741135\n",
            "Iteration : 807     Loss : 237.74968500431817\n",
            "Iteration : 808     Loss : 237.2385522916619\n",
            "Iteration : 809     Loss : 236.7275288589364\n",
            "Iteration : 810     Loss : 236.21661587620434\n",
            "Iteration : 811     Loss : 235.70581452420626\n",
            "Iteration : 812     Loss : 235.19512599447327\n",
            "Iteration : 813     Loss : 234.68455148943676\n",
            "Iteration : 814     Loss : 234.17409222253974\n",
            "Iteration : 815     Loss : 233.66374941835056\n",
            "Iteration : 816     Loss : 233.15352431267712\n",
            "Iteration : 817     Loss : 232.64341815267855\n",
            "Iteration : 818     Loss : 232.13343219698183\n",
            "Iteration : 819     Loss : 231.62356771579636\n",
            "Iteration : 820     Loss : 231.11382599103007\n",
            "Iteration : 821     Loss : 230.60420831640513\n",
            "Iteration : 822     Loss : 230.09471599757873\n",
            "Iteration : 823     Loss : 229.58535035225572\n",
            "Iteration : 824     Loss : 229.0761127103126\n",
            "Iteration : 825     Loss : 228.56700441391484\n",
            "Iteration : 826     Loss : 228.05802681763524\n",
            "Iteration : 827     Loss : 227.54918128857767\n",
            "Iteration : 828     Loss : 227.04046920649785\n",
            "Iteration : 829     Loss : 226.5318919639238\n",
            "Iteration : 830     Loss : 226.02345096628142\n",
            "Iteration : 831     Loss : 225.51514763201513\n",
            "Iteration : 832     Loss : 225.006983392715\n",
            "Iteration : 833     Loss : 224.49895969323902\n",
            "Iteration : 834     Loss : 223.9910779918404\n",
            "Iteration : 835     Loss : 223.48333976029357\n",
            "Iteration : 836     Loss : 222.97574648402033\n",
            "Iteration : 837     Loss : 222.46829966221873\n",
            "Iteration : 838     Loss : 221.96100080798914\n",
            "Iteration : 839     Loss : 221.45385144846696\n",
            "Iteration : 840     Loss : 220.9468531249475\n",
            "Iteration : 841     Loss : 220.4400073930201\n",
            "Iteration : 842     Loss : 219.93331582269687\n",
            "Iteration : 843     Loss : 219.4267799985457\n",
            "Iteration : 844     Loss : 218.92040151981985\n",
            "Iteration : 845     Loss : 218.41418200059513\n",
            "Iteration : 846     Loss : 217.9081230698989\n",
            "Iteration : 847     Loss : 217.4022263718468\n",
            "Iteration : 848     Loss : 216.89649356577698\n",
            "Iteration : 849     Loss : 216.3909263263845\n",
            "Iteration : 850     Loss : 215.8855263438588\n",
            "Iteration : 851     Loss : 215.38029532401896\n",
            "Iteration : 852     Loss : 214.8752349884512\n",
            "Iteration : 853     Loss : 214.3703470746469\n",
            "Iteration : 854     Loss : 213.8656333361413\n",
            "Iteration : 855     Loss : 213.36109554265067\n",
            "Iteration : 856     Loss : 212.8567354802135\n",
            "Iteration : 857     Loss : 212.35255495132998\n",
            "Iteration : 858     Loss : 211.8485557751021\n",
            "Iteration : 859     Loss : 211.3447397873753\n",
            "Iteration : 860     Loss : 210.84110884087974\n",
            "Iteration : 861     Loss : 210.3376648053722\n",
            "Iteration : 862     Loss : 209.83440956777932\n",
            "Iteration : 863     Loss : 209.33134503234018\n",
            "Iteration : 864     Loss : 208.82847312074972\n",
            "Iteration : 865     Loss : 208.32579577230368\n",
            "Iteration : 866     Loss : 207.82331494404175\n",
            "Iteration : 867     Loss : 207.3210326108935\n",
            "Iteration : 868     Loss : 206.81895076582387\n",
            "Iteration : 869     Loss : 206.31707141997734\n",
            "Iteration : 870     Loss : 205.81539660282635\n",
            "Iteration : 871     Loss : 205.3139283623157\n",
            "Iteration : 872     Loss : 204.812668765011\n",
            "Iteration : 873     Loss : 204.31161989624508\n",
            "Iteration : 874     Loss : 203.81078386026502\n",
            "Iteration : 875     Loss : 203.31016278038177\n",
            "Iteration : 876     Loss : 202.8097587991163\n",
            "Iteration : 877     Loss : 202.30957407834958\n",
            "Iteration : 878     Loss : 201.8096107994697\n",
            "Iteration : 879     Loss : 201.30987116352213\n",
            "Iteration : 880     Loss : 200.81035739135783\n",
            "Iteration : 881     Loss : 200.31107172378395\n",
            "Iteration : 882     Loss : 199.81201642171064\n",
            "Iteration : 883     Loss : 199.3131937663036\n",
            "Iteration : 884     Loss : 198.8146060591318\n",
            "Iteration : 885     Loss : 198.31625562231756\n",
            "Iteration : 886     Loss : 197.81814479868737\n",
            "Iteration : 887     Loss : 197.3202759519211\n",
            "Iteration : 888     Loss : 196.82265146670142\n",
            "Iteration : 889     Loss : 196.32527374886422\n",
            "Iteration : 890     Loss : 195.8281452255487\n",
            "Iteration : 891     Loss : 195.3312683453472\n",
            "Iteration : 892     Loss : 194.83464557845386\n",
            "Iteration : 893     Loss : 194.33827941681545\n",
            "Iteration : 894     Loss : 193.84217237427984\n",
            "Iteration : 895     Loss : 193.34632698674557\n",
            "Iteration : 896     Loss : 192.85074581231117\n",
            "Iteration : 897     Loss : 192.35543143142323\n",
            "Iteration : 898     Loss : 191.8603864470253\n",
            "Iteration : 899     Loss : 191.36561348470585\n",
            "Iteration : 900     Loss : 190.87111519284574\n",
            "Iteration : 901     Loss : 190.37689424276644\n",
            "Iteration : 902     Loss : 189.8829533288754\n",
            "Iteration : 903     Loss : 189.38929516881413\n",
            "Iteration : 904     Loss : 188.89592250360323\n",
            "Iteration : 905     Loss : 188.40283809778742\n",
            "Iteration : 906     Loss : 187.9100447395815\n",
            "Iteration : 907     Loss : 187.41754524101336\n",
            "Iteration : 908     Loss : 186.92534243806764\n",
            "Iteration : 909     Loss : 186.43343919082923\n",
            "Iteration : 910     Loss : 185.9418383836239\n",
            "Iteration : 911     Loss : 185.45054292516065\n",
            "Iteration : 912     Loss : 184.9595557486711\n",
            "Iteration : 913     Loss : 184.46887981204958\n",
            "Iteration : 914     Loss : 183.97851809799076\n",
            "Iteration : 915     Loss : 183.48847361412763\n",
            "Iteration : 916     Loss : 182.9987493931678\n",
            "Iteration : 917     Loss : 182.50934849302817\n",
            "Iteration : 918     Loss : 182.02027399696928\n",
            "Iteration : 919     Loss : 181.53152901372866\n",
            "Iteration : 920     Loss : 181.04311667765157\n",
            "Iteration : 921     Loss : 180.55504014882138\n",
            "Iteration : 922     Loss : 180.06730261318916\n",
            "Iteration : 923     Loss : 179.57990728269976\n",
            "Iteration : 924     Loss : 179.0928573954188\n",
            "Iteration : 925     Loss : 178.60615621565657\n",
            "Iteration : 926     Loss : 178.11980703409043\n",
            "Iteration : 927     Loss : 177.63381316788636\n",
            "Iteration : 928     Loss : 177.14817796081786\n",
            "Iteration : 929     Loss : 176.6629047833832\n",
            "Iteration : 930     Loss : 176.17799703292152\n",
            "Iteration : 931     Loss : 175.69345813372553\n",
            "Iteration : 932     Loss : 175.20929153715434\n",
            "Iteration : 933     Loss : 174.72550072174124\n",
            "Iteration : 934     Loss : 174.24208919330275\n",
            "Iteration : 935     Loss : 173.7590604850423\n",
            "Iteration : 936     Loss : 173.27641815765384\n",
            "Iteration : 937     Loss : 172.79416579942185\n",
            "Iteration : 938     Loss : 172.31230702631925\n",
            "Iteration : 939     Loss : 171.8308454821032\n",
            "Iteration : 940     Loss : 171.3497848384073\n",
            "Iteration : 941     Loss : 170.86912879483177\n",
            "Iteration : 942     Loss : 170.38888107903162\n",
            "Iteration : 943     Loss : 169.90904544679984\n",
            "Iteration : 944     Loss : 169.42962568215012\n",
            "Iteration : 945     Loss : 168.9506255973942\n",
            "Iteration : 946     Loss : 168.4720490332187\n",
            "Iteration : 947     Loss : 167.99389985875618\n",
            "Iteration : 948     Loss : 167.51618197165456\n",
            "Iteration : 949     Loss : 167.03889929814292\n",
            "Iteration : 950     Loss : 166.5620557930934\n",
            "Iteration : 951     Loss : 166.08565544008013\n",
            "Iteration : 952     Loss : 165.60970225143365\n",
            "Iteration : 953     Loss : 165.13420026829291\n",
            "Iteration : 954     Loss : 164.65915356065233\n",
            "Iteration : 955     Loss : 164.1845662274053\n",
            "Iteration : 956     Loss : 163.7104423963838\n",
            "Iteration : 957     Loss : 163.23678622439436\n",
            "Iteration : 958     Loss : 162.76360189724826\n",
            "Iteration : 959     Loss : 162.29089362978945\n",
            "Iteration : 960     Loss : 161.81866566591665\n",
            "Iteration : 961     Loss : 161.3469222786013\n",
            "Iteration : 962     Loss : 160.87566776990155\n",
            "Iteration : 963     Loss : 160.40490647097033\n",
            "Iteration : 964     Loss : 159.93464274205965\n",
            "Iteration : 965     Loss : 159.46488097251952\n",
            "Iteration : 966     Loss : 158.99562558079245\n",
            "Iteration : 967     Loss : 158.5268810144014\n",
            "Iteration : 968     Loss : 158.0586517499343\n",
            "Iteration : 969     Loss : 157.5909422930219\n",
            "Iteration : 970     Loss : 157.12375717831085\n",
            "Iteration : 971     Loss : 156.65710096943096\n",
            "Iteration : 972     Loss : 156.1909782589569\n",
            "Iteration : 973     Loss : 155.72539366836384\n",
            "Iteration : 974     Loss : 155.26035184797811\n",
            "Iteration : 975     Loss : 154.79585747692022\n",
            "Iteration : 976     Loss : 154.33191526304375\n",
            "Iteration : 977     Loss : 153.8685299428662\n",
            "Iteration : 978     Loss : 153.40570628149476\n",
            "Iteration : 979     Loss : 152.9434490725452\n",
            "Iteration : 980     Loss : 152.481763138054\n",
            "Iteration : 981     Loss : 152.0206533283839\n",
            "Iteration : 982     Loss : 151.560124522123\n",
            "Iteration : 983     Loss : 151.10018162597675\n",
            "Iteration : 984     Loss : 150.6408295746523\n",
            "Iteration : 985     Loss : 150.18207333073693\n",
            "Iteration : 986     Loss : 149.723917884568\n",
            "Iteration : 987     Loss : 149.26636825409665\n",
            "Iteration : 988     Loss : 148.8094294847429\n",
            "Iteration : 989     Loss : 148.3531066492439\n",
            "Iteration : 990     Loss : 147.8974048474944\n",
            "Iteration : 991     Loss : 147.4423292063793\n",
            "Iteration : 992     Loss : 146.98788487959763\n",
            "Iteration : 993     Loss : 146.53407704747988\n",
            "Iteration : 994     Loss : 146.0809109167956\n",
            "Iteration : 995     Loss : 145.6283917205538\n",
            "Iteration : 996     Loss : 145.17652471779456\n",
            "Iteration : 997     Loss : 144.72531519337207\n",
            "Iteration : 998     Loss : 144.27476845772907\n",
            "Iteration : 999     Loss : 143.82488984666273\n",
            "Iteration : 1000     Loss : 143.37568472108165\n",
            "Iteration : 1001     Loss : 142.92715846675353\n",
            "Iteration : 1002     Loss : 142.47931649404433\n",
            "Iteration : 1003     Loss : 142.03216423764815\n",
            "Iteration : 1004     Loss : 141.5857071563071\n",
            "Iteration : 1005     Loss : 141.13995073252337\n",
            "Iteration : 1006     Loss : 140.69490047225983\n",
            "Iteration : 1007     Loss : 140.2505619046329\n",
            "Iteration : 1008     Loss : 139.8069405815945\n",
            "Iteration : 1009     Loss : 139.36404207760472\n",
            "Iteration : 1010     Loss : 138.92187198929494\n",
            "Iteration : 1011     Loss : 138.48043593511994\n",
            "Iteration : 1012     Loss : 138.0397395550013\n",
            "Iteration : 1013     Loss : 137.59978850995975\n",
            "Iteration : 1014     Loss : 137.1605884817375\n",
            "Iteration : 1015     Loss : 136.72214517241014\n",
            "Iteration : 1016     Loss : 136.28446430398904\n",
            "Iteration : 1017     Loss : 135.84755161801166\n",
            "Iteration : 1018     Loss : 135.4114128751228\n",
            "Iteration : 1019     Loss : 134.97605385464453\n",
            "Iteration : 1020     Loss : 134.54148035413536\n",
            "Iteration : 1021     Loss : 134.10769818893917\n",
            "Iteration : 1022     Loss : 133.6747131917228\n",
            "Iteration : 1023     Loss : 133.24253121200283\n",
            "Iteration : 1024     Loss : 132.81115811566187\n",
            "Iteration : 1025     Loss : 132.3805997844533\n",
            "Iteration : 1026     Loss : 131.95086211549506\n",
            "Iteration : 1027     Loss : 131.521951020753\n",
            "Iteration : 1028     Loss : 131.09387242651212\n",
            "Iteration : 1029     Loss : 130.6666322728372\n",
            "Iteration : 1030     Loss : 130.24023651302224\n",
            "Iteration : 1031     Loss : 129.81469111302846\n",
            "Iteration : 1032     Loss : 129.39000205091153\n",
            "Iteration : 1033     Loss : 128.9661753162363\n",
            "Iteration : 1034     Loss : 128.5432169094819\n",
            "Iteration : 1035     Loss : 128.1211328414343\n",
            "Iteration : 1036     Loss : 127.69992913256809\n",
            "Iteration : 1037     Loss : 127.27961181241669\n",
            "Iteration : 1038     Loss : 126.8601869189315\n",
            "Iteration : 1039     Loss : 126.44166049782969\n",
            "Iteration : 1040     Loss : 126.02403860193046\n",
            "Iteration : 1041     Loss : 125.60732729048036\n",
            "Iteration : 1042     Loss : 125.19153262846723\n",
            "Iteration : 1043     Loss : 124.77666068592283\n",
            "Iteration : 1044     Loss : 124.36271753721465\n",
            "Iteration : 1045     Loss : 123.94970926032589\n",
            "Iteration : 1046     Loss : 123.5376419361256\n",
            "Iteration : 1047     Loss : 123.12652164762623\n",
            "Iteration : 1048     Loss : 122.71635447923163\n",
            "Iteration : 1049     Loss : 122.30714651597326\n",
            "Iteration : 1050     Loss : 121.89890384273605\n",
            "Iteration : 1051     Loss : 121.49163254347327\n",
            "Iteration : 1052     Loss : 121.08533870041104\n",
            "Iteration : 1053     Loss : 120.68002839324198\n",
            "Iteration : 1054     Loss : 120.27570769830864\n",
            "Iteration : 1055     Loss : 119.87238268777683\n",
            "Iteration : 1056     Loss : 119.47005942879794\n",
            "Iteration : 1057     Loss : 119.06874398266211\n",
            "Iteration : 1058     Loss : 118.66844240394096\n",
            "Iteration : 1059     Loss : 118.26916073962036\n",
            "Iteration : 1060     Loss : 117.87090502822421\n",
            "Iteration : 1061     Loss : 117.47368129892779\n",
            "Iteration : 1062     Loss : 117.07749557066272\n",
            "Iteration : 1063     Loss : 116.68235385121172\n",
            "Iteration : 1064     Loss : 116.28826213629534\n",
            "Iteration : 1065     Loss : 115.89522640864931\n",
            "Iteration : 1066     Loss : 115.50325263709344\n",
            "Iteration : 1067     Loss : 115.11234677559202\n",
            "Iteration : 1068     Loss : 114.72251476230585\n",
            "Iteration : 1069     Loss : 114.33376251863706\n",
            "Iteration : 1070     Loss : 113.94609594826487\n",
            "Iteration : 1071     Loss : 113.55952093617503\n",
            "Iteration : 1072     Loss : 113.17404334768138\n",
            "Iteration : 1073     Loss : 112.78966902744004\n",
            "Iteration : 1074     Loss : 112.40640379845765\n",
            "Iteration : 1075     Loss : 112.02425346109236\n",
            "Iteration : 1076     Loss : 111.64322379204903\n",
            "Iteration : 1077     Loss : 111.26332054336794\n",
            "Iteration : 1078     Loss : 110.88454944140835\n",
            "Iteration : 1079     Loss : 110.50691618582644\n",
            "Iteration : 1080     Loss : 110.13042644854777\n",
            "Iteration : 1081     Loss : 109.75508587273579\n",
            "Iteration : 1082     Loss : 109.3809000717549\n",
            "Iteration : 1083     Loss : 109.00787462813037\n",
            "Iteration : 1084     Loss : 108.63601509250354\n",
            "Iteration : 1085     Loss : 108.26532698258399\n",
            "Iteration : 1086     Loss : 107.89581578209882\n",
            "Iteration : 1087     Loss : 107.52748693973838\n",
            "Iteration : 1088     Loss : 107.16034586810042\n",
            "Iteration : 1089     Loss : 106.79439794263169\n",
            "Iteration : 1090     Loss : 106.42964850056796\n",
            "Iteration : 1091     Loss : 106.06610283987281\n",
            "Iteration : 1092     Loss : 105.70376621817543\n",
            "Iteration : 1093     Loss : 105.34264385170803\n",
            "Iteration : 1094     Loss : 104.98274091424267\n",
            "Iteration : 1095     Loss : 104.62406253602933\n",
            "Iteration : 1096     Loss : 104.26661380273332\n",
            "Iteration : 1097     Loss : 103.91039975437505\n",
            "Iteration : 1098     Loss : 103.55542538427055\n",
            "Iteration : 1099     Loss : 103.20169563797405\n",
            "Iteration : 1100     Loss : 102.84921541222317\n",
            "Iteration : 1101     Loss : 102.49798955388673\n",
            "Iteration : 1102     Loss : 102.1480228589158\n",
            "Iteration : 1103     Loss : 101.79932007129817\n",
            "Iteration : 1104     Loss : 101.45188588201768\n",
            "Iteration : 1105     Loss : 101.10572492801735\n",
            "Iteration : 1106     Loss : 100.7608417911676\n",
            "Iteration : 1107     Loss : 100.41724099724044\n",
            "Iteration : 1108     Loss : 100.07492701488925\n",
            "Iteration : 1109     Loss : 99.7339042546345\n",
            "Iteration : 1110     Loss : 99.39417706785727\n",
            "Iteration : 1111     Loss : 99.05574974579925\n",
            "Iteration : 1112     Loss : 98.71862651857082\n",
            "Iteration : 1113     Loss : 98.38281155416719\n",
            "Iteration : 1114     Loss : 98.04830895749338\n",
            "Iteration : 1115     Loss : 97.71512276939822\n",
            "Iteration : 1116     Loss : 97.38325696571813\n",
            "Iteration : 1117     Loss : 97.0527154563304\n",
            "Iteration : 1118     Loss : 96.72350208421797\n",
            "Iteration : 1119     Loss : 96.39562062454404\n",
            "Iteration : 1120     Loss : 96.06907478373897\n",
            "Iteration : 1121     Loss : 95.74386819859878\n",
            "Iteration : 1122     Loss : 95.42000443539587\n",
            "Iteration : 1123     Loss : 95.09748698900265\n",
            "Iteration : 1124     Loss : 94.77631928202845\n",
            "Iteration : 1125     Loss : 94.4565046639701\n",
            "Iteration : 1126     Loss : 94.13804641037636\n",
            "Iteration : 1127     Loss : 93.82094772202731\n",
            "Iteration : 1128     Loss : 93.50521172412844\n",
            "Iteration : 1129     Loss : 93.19084146552032\n",
            "Iteration : 1130     Loss : 92.87783991790384\n",
            "Iteration : 1131     Loss : 92.56620997508216\n",
            "Iteration : 1132     Loss : 92.25595445221867\n",
            "Iteration : 1133     Loss : 91.94707608511257\n",
            "Iteration : 1134     Loss : 91.63957752949158\n",
            "Iteration : 1135     Loss : 91.33346136032252\n",
            "Iteration : 1136     Loss : 91.02873007114039\n",
            "Iteration : 1137     Loss : 90.72538607339546\n",
            "Iteration : 1138     Loss : 90.4234316958197\n",
            "Iteration : 1139     Loss : 90.12286918381278\n",
            "Iteration : 1140     Loss : 89.82370069884696\n",
            "Iteration : 1141     Loss : 89.52592831789264\n",
            "Iteration : 1142     Loss : 89.22955403286377\n",
            "Iteration : 1143     Loss : 88.93457975008434\n",
            "Iteration : 1144     Loss : 88.64100728977526\n",
            "Iteration : 1145     Loss : 88.34883838556311\n",
            "Iteration : 1146     Loss : 88.05807468400964\n",
            "Iteration : 1147     Loss : 87.76871774416375\n",
            "Iteration : 1148     Loss : 87.48076903713516\n",
            "Iteration : 1149     Loss : 87.19422994569048\n",
            "Iteration : 1150     Loss : 86.9091017638717\n",
            "Iteration : 1151     Loss : 86.62538569663776\n",
            "Iteration : 1152     Loss : 86.34308285952868\n",
            "Iteration : 1153     Loss : 86.0621942783532\n",
            "Iteration : 1154     Loss : 85.7827208888997\n",
            "Iteration : 1155     Loss : 85.50466353667053\n",
            "Iteration : 1156     Loss : 85.22802297664047\n",
            "Iteration : 1157     Loss : 84.95279987303834\n",
            "Iteration : 1158     Loss : 84.6789947991534\n",
            "Iteration : 1159     Loss : 84.4066082371653\n",
            "Iteration : 1160     Loss : 84.13564057799853\n",
            "Iteration : 1161     Loss : 83.8660921212011\n",
            "Iteration : 1162     Loss : 83.59796307484774\n",
            "Iteration : 1163     Loss : 83.33125355546719\n",
            "Iteration : 1164     Loss : 83.06596358799483\n",
            "Iteration : 1165     Loss : 82.80209310574872\n",
            "Iteration : 1166     Loss : 82.53964195043146\n",
            "Iteration : 1167     Loss : 82.27860987215551\n",
            "Iteration : 1168     Loss : 82.01899652949409\n",
            "Iteration : 1169     Loss : 81.76080148955566\n",
            "Iteration : 1170     Loss : 81.50402422808405\n",
            "Iteration : 1171     Loss : 81.24866412958202\n",
            "Iteration : 1172     Loss : 80.99472048746004\n",
            "Iteration : 1173     Loss : 80.74219250420887\n",
            "Iteration : 1174     Loss : 80.49107929159672\n",
            "Iteration : 1175     Loss : 80.2413798708905\n",
            "Iteration : 1176     Loss : 79.993093173101\n",
            "Iteration : 1177     Loss : 79.74621803925218\n",
            "Iteration : 1178     Loss : 79.50075322067386\n",
            "Iteration : 1179     Loss : 79.2566973793184\n",
            "Iteration : 1180     Loss : 79.01404908810076\n",
            "Iteration : 1181     Loss : 78.77280683126128\n",
            "Iteration : 1182     Loss : 78.5329690047522\n",
            "Iteration : 1183     Loss : 78.29453391664669\n",
            "Iteration : 1184     Loss : 78.05749978757024\n",
            "Iteration : 1185     Loss : 77.82186475115479\n",
            "Iteration : 1186     Loss : 77.58762685451494\n",
            "Iteration : 1187     Loss : 77.35478405874576\n",
            "Iteration : 1188     Loss : 77.12333423944223\n",
            "Iteration : 1189     Loss : 76.89327518724015\n",
            "Iteration : 1190     Loss : 76.66460460837803\n",
            "Iteration : 1191     Loss : 76.4373201252793\n",
            "Iteration : 1192     Loss : 76.21141927715544\n",
            "Iteration : 1193     Loss : 75.98689952062865\n",
            "Iteration : 1194     Loss : 75.76375823037456\n",
            "Iteration : 1195     Loss : 75.54199269978399\n",
            "Iteration : 1196     Loss : 75.32160014164408\n",
            "Iteration : 1197     Loss : 75.1025776888377\n",
            "Iteration : 1198     Loss : 74.88492239506111\n",
            "Iteration : 1199     Loss : 74.66863123556001\n",
            "Iteration : 1200     Loss : 74.45370110788215\n",
            "Iteration : 1201     Loss : 74.24012883264794\n",
            "Iteration : 1202     Loss : 74.02791115433718\n",
            "Iteration : 1203     Loss : 73.81704474209187\n",
            "Iteration : 1204     Loss : 73.60752619053568\n",
            "Iteration : 1205     Loss : 73.39935202060796\n",
            "Iteration : 1206     Loss : 73.192518680413\n",
            "Iteration : 1207     Loss : 72.98702254608413\n",
            "Iteration : 1208     Loss : 72.78285992266146\n",
            "Iteration : 1209     Loss : 72.58002704498351\n",
            "Iteration : 1210     Loss : 72.37852007859192\n",
            "Iteration : 1211     Loss : 72.17833512064907\n",
            "Iteration : 1212     Loss : 71.97946820086774\n",
            "Iteration : 1213     Loss : 71.7819152824528\n",
            "Iteration : 1214     Loss : 71.58567226305397\n",
            "Iteration : 1215     Loss : 71.39073497572976\n",
            "Iteration : 1216     Loss : 71.19709918992174\n",
            "Iteration : 1217     Loss : 71.0047606124385\n",
            "Iteration : 1218     Loss : 70.81371488844944\n",
            "Iteration : 1219     Loss : 70.62395760248728\n",
            "Iteration : 1220     Loss : 70.43548427945919\n",
            "Iteration : 1221     Loss : 70.2482903856661\n",
            "Iteration : 1222     Loss : 70.06237132982923\n",
            "Iteration : 1223     Loss : 69.87772246412403\n",
            "Iteration : 1224     Loss : 69.69433908522066\n",
            "Iteration : 1225     Loss : 69.51221643533056\n",
            "Iteration : 1226     Loss : 69.33134970325875\n",
            "Iteration : 1227     Loss : 69.15173402546147\n",
            "Iteration : 1228     Loss : 68.97336448710817\n",
            "Iteration : 1229     Loss : 68.79623612314828\n",
            "Iteration : 1230     Loss : 68.62034391938164\n",
            "Iteration : 1231     Loss : 68.44568281353199\n",
            "Iteration : 1232     Loss : 68.27224769632394\n",
            "Iteration : 1233     Loss : 68.10003341256193\n",
            "Iteration : 1234     Loss : 67.92903476221156\n",
            "Iteration : 1235     Loss : 67.75924650148211\n",
            "Iteration : 1236     Loss : 67.59066334391078\n",
            "Iteration : 1237     Loss : 67.423279961447\n",
            "Iteration : 1238     Loss : 67.25709098553753\n",
            "Iteration : 1239     Loss : 67.09209100821123\n",
            "Iteration : 1240     Loss : 66.92827458316336\n",
            "Iteration : 1241     Loss : 66.76563622683867\n",
            "Iteration : 1242     Loss : 66.60417041951361\n",
            "Iteration : 1243     Loss : 66.44387160637643\n",
            "Iteration : 1244     Loss : 66.284734198605\n",
            "Iteration : 1245     Loss : 66.1267525744428\n",
            "Iteration : 1246     Loss : 65.96992108027105\n",
            "Iteration : 1247     Loss : 65.81423403167837\n",
            "Iteration : 1248     Loss : 65.65968571452642\n",
            "Iteration : 1249     Loss : 65.50627038601145\n",
            "Iteration : 1250     Loss : 65.3539822757219\n",
            "Iteration : 1251     Loss : 65.20281558669089\n",
            "Iteration : 1252     Loss : 65.05276449644407\n",
            "Iteration : 1253     Loss : 64.9038231580418\n",
            "Iteration : 1254     Loss : 64.7559857011158\n",
            "Iteration : 1255     Loss : 64.60924623289999\n",
            "Iteration : 1256     Loss : 64.4635988392548\n",
            "Iteration : 1257     Loss : 64.319037585685\n",
            "Iteration : 1258     Loss : 64.17555651835072\n",
            "Iteration : 1259     Loss : 64.0331496650714\n",
            "Iteration : 1260     Loss : 63.89181103632222\n",
            "Iteration : 1261     Loss : 63.751534626222714\n",
            "Iteration : 1262     Loss : 63.61231441351794\n",
            "Iteration : 1263     Loss : 63.47414436255107\n",
            "Iteration : 1264     Loss : 63.33701842422768\n",
            "Iteration : 1265     Loss : 63.20093053697151\n",
            "Iteration : 1266     Loss : 63.06587462767138\n",
            "Iteration : 1267     Loss : 62.93184461261885\n",
            "Iteration : 1268     Loss : 62.79883439843692\n",
            "Iteration : 1269     Loss : 62.66683788299897\n",
            "Iteration : 1270     Loss : 62.53584895633835\n",
            "Iteration : 1271     Loss : 62.405861501547875\n",
            "Iteration : 1272     Loss : 62.276869395669586\n",
            "Iteration : 1273     Loss : 62.14886651057414\n",
            "Iteration : 1274     Loss : 62.02184671382997\n",
            "Iteration : 1275     Loss : 61.895803869562165\n",
            "Iteration : 1276     Loss : 61.770731839300396\n",
            "Iteration : 1277     Loss : 61.64662448281638\n",
            "Iteration : 1278     Loss : 61.52347565895058\n",
            "Iteration : 1279     Loss : 61.40127922642765\n",
            "Iteration : 1280     Loss : 61.280029044660964\n",
            "Iteration : 1281     Loss : 61.15971897454606\n",
            "Iteration : 1282     Loss : 61.04034287924269\n",
            "Iteration : 1283     Loss : 60.92189462494558\n",
            "Iteration : 1284     Loss : 60.80436808164378\n",
            "Iteration : 1285     Loss : 60.687757123868444\n",
            "Iteration : 1286     Loss : 60.57205563142912\n",
            "Iteration : 1287     Loss : 60.45725749013849\n",
            "Iteration : 1288     Loss : 60.34335659252512\n",
            "Iteration : 1289     Loss : 60.23034683853505\n",
            "Iteration : 1290     Loss : 60.118222136221014\n",
            "Iteration : 1291     Loss : 60.00697640242038\n",
            "Iteration : 1292     Loss : 59.89660356342102\n",
            "Iteration : 1293     Loss : 59.78709755561539\n",
            "Iteration : 1294     Loss : 59.678452326142846\n",
            "Iteration : 1295     Loss : 59.57066183352008\n",
            "Iteration : 1296     Loss : 59.463720048259646\n",
            "Iteration : 1297     Loss : 59.35762095347678\n",
            "Iteration : 1298     Loss : 59.25235854548426\n",
            "Iteration : 1299     Loss : 59.14792683437558\n",
            "Iteration : 1300     Loss : 59.04431984459611\n",
            "Iteration : 1301     Loss : 58.94153161550285\n",
            "Iteration : 1302     Loss : 58.83955620191204\n",
            "Iteration : 1303     Loss : 58.738387674635334\n",
            "Iteration : 1304     Loss : 58.638020121004296\n",
            "Iteration : 1305     Loss : 58.53844764538307\n",
            "Iteration : 1306     Loss : 58.43966436966981\n",
            "Iteration : 1307     Loss : 58.34166443378627\n",
            "Iteration : 1308     Loss : 58.24444199615609\n",
            "Iteration : 1309     Loss : 58.14799123417186\n",
            "Iteration : 1310     Loss : 58.05230634465037\n",
            "Iteration : 1311     Loss : 57.95738154427712\n",
            "Iteration : 1312     Loss : 57.86321107003929\n",
            "Iteration : 1313     Loss : 57.769789179647766\n",
            "Iteration : 1314     Loss : 57.67711015194807\n",
            "Iteration : 1315     Loss : 57.58516828732031\n",
            "Iteration : 1316     Loss : 57.4939579080683\n",
            "Iteration : 1317     Loss : 57.40347335879793\n",
            "Iteration : 1318     Loss : 57.31370900678474\n",
            "Iteration : 1319     Loss : 57.22465924233099\n",
            "Iteration : 1320     Loss : 57.13631847911216\n",
            "Iteration : 1321     Loss : 57.04868115451318\n",
            "Iteration : 1322     Loss : 56.9617417299541\n",
            "Iteration : 1323     Loss : 56.875494691205844\n",
            "Iteration : 1324     Loss : 56.789934548695726\n",
            "Iteration : 1325     Loss : 56.70505583780295\n",
            "Iteration : 1326     Loss : 56.620853119144286\n",
            "Iteration : 1327     Loss : 56.53732097884996\n",
            "Iteration : 1328     Loss : 56.454454028829915\n",
            "Iteration : 1329     Loss : 56.372246907030544\n",
            "Iteration : 1330     Loss : 56.29069427768178\n",
            "Iteration : 1331     Loss : 56.209790831535244\n",
            "Iteration : 1332     Loss : 56.12953128609293\n",
            "Iteration : 1333     Loss : 56.049910385826735\n",
            "Iteration : 1334     Loss : 55.97092290238944\n",
            "Iteration : 1335     Loss : 55.8925636348164\n",
            "Iteration : 1336     Loss : 55.81482740971865\n",
            "Iteration : 1337     Loss : 55.737709081467735\n",
            "Iteration : 1338     Loss : 55.66120353237152\n",
            "Iteration : 1339     Loss : 55.58530567284211\n",
            "Iteration : 1340     Loss : 55.510010441555174\n",
            "Iteration : 1341     Loss : 55.43531280560167\n",
            "Iteration : 1342     Loss : 55.36120776063096\n",
            "Iteration : 1343     Loss : 55.287690330986564\n",
            "Iteration : 1344     Loss : 55.21475556983396\n",
            "Iteration : 1345     Loss : 55.14239855928088\n",
            "Iteration : 1346     Loss : 55.07061441048996\n",
            "Iteration : 1347     Loss : 54.99939826378429\n",
            "Iteration : 1348     Loss : 54.928745288745475\n",
            "Iteration : 1349     Loss : 54.858650684304834\n",
            "Iteration : 1350     Loss : 54.78910967882736\n",
            "Iteration : 1351     Loss : 54.720117530189015\n",
            "Iteration : 1352     Loss : 54.651669525847296\n",
            "Iteration : 1353     Loss : 54.5837609829049\n",
            "Iteration : 1354     Loss : 54.51638724816735\n",
            "Iteration : 1355     Loss : 54.44954369819399\n",
            "Iteration : 1356     Loss : 54.383225739342606\n",
            "Iteration : 1357     Loss : 54.31742880780825\n",
            "Iteration : 1358     Loss : 54.252148369655814\n",
            "Iteration : 1359     Loss : 54.187379920846816\n",
            "Iteration : 1360     Loss : 54.123118987260334\n",
            "Iteration : 1361     Loss : 54.059361124708495\n",
            "Iteration : 1362     Loss : 53.996101918946174\n",
            "Iteration : 1363     Loss : 53.93333698567544\n",
            "Iteration : 1364     Loss : 53.871061970544616\n",
            "Iteration : 1365     Loss : 53.80927254914236\n",
            "Iteration : 1366     Loss : 53.74796442698623\n",
            "Iteration : 1367     Loss : 53.687133339506886\n",
            "Iteration : 1368     Loss : 53.62677505202689\n",
            "Iteration : 1369     Loss : 53.56688535973514\n",
            "Iteration : 1370     Loss : 53.50746008765652\n",
            "Iteration : 1371     Loss : 53.44849509061704\n",
            "Iteration : 1372     Loss : 53.389986253204576\n",
            "Iteration : 1373     Loss : 53.33192948972544\n",
            "Iteration : 1374     Loss : 53.274320744156455\n",
            "Iteration : 1375     Loss : 53.21715599009316\n",
            "Iteration : 1376     Loss : 53.160431230694016\n",
            "Iteration : 1377     Loss : 53.104142498620696\n",
            "Iteration : 1378     Loss : 53.04828585597441\n",
            "Iteration : 1379     Loss : 52.99285739422887\n",
            "Iteration : 1380     Loss : 52.937853234159235\n",
            "Iteration : 1381     Loss : 52.88326952576823\n",
            "Iteration : 1382     Loss : 52.82910244820789\n",
            "Iteration : 1383     Loss : 52.77534820969903\n",
            "Iteration : 1384     Loss : 52.72200304744666\n",
            "Iteration : 1385     Loss : 52.66906322755289\n",
            "Iteration : 1386     Loss : 52.61652504492632\n",
            "Iteration : 1387     Loss : 52.56438482318889\n",
            "Iteration : 1388     Loss : 52.512638914579554\n",
            "Iteration : 1389     Loss : 52.461283699855535\n",
            "Iteration : 1390     Loss : 52.410315588190336\n",
            "Iteration : 1391     Loss : 52.359731017069876\n",
            "Iteration : 1392     Loss : 52.30952645218542\n",
            "Iteration : 1393     Loss : 52.2596983873247\n",
            "Iteration : 1394     Loss : 52.21024334426002\n",
            "Iteration : 1395     Loss : 52.161157872634746\n",
            "Iteration : 1396     Loss : 52.112438549847056\n",
            "Iteration : 1397     Loss : 52.064081980931924\n",
            "Iteration : 1398     Loss : 52.016084798440865\n",
            "Iteration : 1399     Loss : 51.96844366231972\n",
            "Iteration : 1400     Loss : 51.921155259784804\n",
            "Iteration : 1401     Loss : 51.87421630519666\n",
            "Iteration : 1402     Loss : 51.82762353993269\n",
            "Iteration : 1403     Loss : 51.7813737322576\n",
            "Iteration : 1404     Loss : 51.735463677192605\n",
            "Iteration : 1405     Loss : 51.68989019638269\n",
            "Iteration : 1406     Loss : 51.644650137962714\n",
            "Iteration : 1407     Loss : 51.5997403764219\n",
            "Iteration : 1408     Loss : 51.55515781246706\n",
            "Iteration : 1409     Loss : 51.51089937288423\n",
            "Iteration : 1410     Loss : 51.46696201039976\n",
            "Iteration : 1411     Loss : 51.42334270353917\n",
            "Iteration : 1412     Loss : 51.38003845648589\n",
            "Iteration : 1413     Loss : 51.33704629893832\n",
            "Iteration : 1414     Loss : 51.29436328596593\n",
            "Iteration : 1415     Loss : 51.2519864978648\n",
            "Iteration : 1416     Loss : 51.209913040011735\n",
            "Iteration : 1417     Loss : 51.168140042717845\n",
            "Iteration : 1418     Loss : 51.12666466108108\n",
            "Iteration : 1419     Loss : 51.08548407483844\n",
            "Iteration : 1420     Loss : 51.044595488216856\n",
            "Iteration : 1421     Loss : 51.00399612978408\n",
            "Iteration : 1422     Loss : 50.96368325229836\n",
            "Iteration : 1423     Loss : 50.92365413255795\n",
            "Iteration : 1424     Loss : 50.88390607124995\n",
            "Iteration : 1425     Loss : 50.84443639279867\n",
            "Iteration : 1426     Loss : 50.80524244521367\n",
            "Iteration : 1427     Loss : 50.76632159993714\n",
            "Iteration : 1428     Loss : 50.727671251691234\n",
            "Iteration : 1429     Loss : 50.68928881832491\n",
            "Iteration : 1430     Loss : 50.65117174066054\n",
            "Iteration : 1431     Loss : 50.6133174823403\n",
            "Iteration : 1432     Loss : 50.575723529672345\n",
            "Iteration : 1433     Loss : 50.53838739147689\n",
            "Iteration : 1434     Loss : 50.50130659893187\n",
            "Iteration : 1435     Loss : 50.46447870541905\n",
            "Iteration : 1436     Loss : 50.427901286369554\n",
            "Iteration : 1437     Loss : 50.39157193910959\n",
            "Iteration : 1438     Loss : 50.355488282706204\n",
            "Iteration : 1439     Loss : 50.319647957813046\n",
            "Iteration : 1440     Loss : 50.284048626516075\n",
            "Iteration : 1441     Loss : 50.2486879721796\n",
            "Iteration : 1442     Loss : 50.21356369929223\n",
            "Iteration : 1443     Loss : 50.178673533313024\n",
            "Iteration : 1444     Loss : 50.144015220517886\n",
            "Iteration : 1445     Loss : 50.10958652784624\n",
            "Iteration : 1446     Loss : 50.075385242747565\n",
            "Iteration : 1447     Loss : 50.041409173028654\n",
            "Iteration : 1448     Loss : 50.007656146700825\n",
            "Iteration : 1449     Loss : 49.97412401182761\n",
            "Iteration : 1450     Loss : 49.940810636372674\n",
            "Iteration : 1451     Loss : 49.907713908048095\n",
            "Iteration : 1452     Loss : 49.87483173416306\n",
            "Iteration : 1453     Loss : 49.84216204147274\n",
            "Iteration : 1454     Loss : 49.809702776028146\n",
            "Iteration : 1455     Loss : 49.77745190302557\n",
            "Iteration : 1456     Loss : 49.745407406657165\n",
            "Iteration : 1457     Loss : 49.71356728996162\n",
            "Iteration : 1458     Loss : 49.68192957467557\n",
            "Iteration : 1459     Loss : 49.65049230108524\n",
            "Iteration : 1460     Loss : 49.61925352787874\n",
            "Iteration : 1461     Loss : 49.58821133199906\n",
            "Iteration : 1462     Loss : 49.55736380849713\n",
            "Iteration : 1463     Loss : 49.526709070386026\n",
            "Iteration : 1464     Loss : 49.49624524849527\n",
            "Iteration : 1465     Loss : 49.46597049132611\n",
            "Iteration : 1466     Loss : 49.43588296490696\n",
            "Iteration : 1467     Loss : 49.40598085265001\n",
            "Iteration : 1468     Loss : 49.3762623552079\n",
            "Iteration : 1469     Loss : 49.346725690331404\n",
            "Iteration : 1470     Loss : 49.31736909272768\n",
            "Iteration : 1471     Loss : 49.288190813918995\n",
            "Iteration : 1472     Loss : 49.25918912210252\n",
            "Iteration : 1473     Loss : 49.230362302010356\n",
            "Iteration : 1474     Loss : 49.20170865477055\n",
            "Iteration : 1475     Loss : 49.173226497768724\n",
            "Iteration : 1476     Loss : 49.1449141645103\n",
            "Iteration : 1477     Loss : 49.11677000448375\n",
            "Iteration : 1478     Loss : 49.08879238302419\n",
            "Iteration : 1479     Loss : 49.06097968117787\n",
            "Iteration : 1480     Loss : 49.03333029556763\n",
            "Iteration : 1481     Loss : 49.00584263825865\n",
            "Iteration : 1482     Loss : 48.97851513662528\n",
            "Iteration : 1483     Loss : 48.951346233218686\n",
            "Iteration : 1484     Loss : 48.92433438563481\n",
            "Iteration : 1485     Loss : 48.89747806638366\n",
            "Iteration : 1486     Loss : 48.87077576275902\n",
            "Iteration : 1487     Loss : 48.84422597670902\n",
            "Iteration : 1488     Loss : 48.81782722470757\n",
            "Iteration : 1489     Loss : 48.79157803762651\n",
            "Iteration : 1490     Loss : 48.76547696060847\n",
            "Iteration : 1491     Loss : 48.73952255294072\n",
            "Iteration : 1492     Loss : 48.71371338792972\n",
            "Iteration : 1493     Loss : 48.68804805277635\n",
            "Iteration : 1494     Loss : 48.66252514845224\n",
            "Iteration : 1495     Loss : 48.63714328957656\n",
            "Iteration : 1496     Loss : 48.6119011042939\n",
            "Iteration : 1497     Loss : 48.586797234152826\n",
            "Iteration : 1498     Loss : 48.561830333985334\n",
            "Iteration : 1499     Loss : 48.53699907178684\n",
            "Iteration : 1500     Loss : 48.51230212859747\n",
            "Iteration : 1501     Loss : 48.48773819838382\n",
            "Iteration : 1502     Loss : 48.46330598792158\n",
            "Iteration : 1503     Loss : 48.439004216679024\n",
            "Iteration : 1504     Loss : 48.414831616701335\n",
            "Iteration : 1505     Loss : 48.390786932495715\n",
            "Iteration : 1506     Loss : 48.366868920917355\n",
            "Iteration : 1507     Loss : 48.34307635105616\n",
            "Iteration : 1508     Loss : 48.31940800412442\n",
            "Iteration : 1509     Loss : 48.29586267334508\n",
            "Iteration : 1510     Loss : 48.272439163840964\n",
            "Iteration : 1511     Loss : 48.249136292525066\n",
            "Iteration : 1512     Loss : 48.225952887991085\n",
            "Iteration : 1513     Loss : 48.20288779040523\n",
            "Iteration : 1514     Loss : 48.179939851398835\n",
            "Iteration : 1515     Loss : 48.15710793396144\n",
            "Iteration : 1516     Loss : 48.1343909123351\n",
            "Iteration : 1517     Loss : 48.111787671909134\n",
            "Iteration : 1518     Loss : 48.08929710911606\n",
            "Iteration : 1519     Loss : 48.06691813132797\n",
            "Iteration : 1520     Loss : 48.04464965675396\n",
            "Iteration : 1521     Loss : 48.02249061433828\n",
            "Iteration : 1522     Loss : 48.00043994365928\n",
            "Iteration : 1523     Loss : 47.978496594829146\n",
            "Iteration : 1524     Loss : 47.9566595283944\n",
            "Iteration : 1525     Loss : 47.934927715237464\n",
            "Iteration : 1526     Loss : 47.9133001364785\n",
            "Iteration : 1527     Loss : 47.89177578337849\n",
            "Iteration : 1528     Loss : 47.87035365724292\n",
            "Iteration : 1529     Loss : 47.84903276932628\n",
            "Iteration : 1530     Loss : 47.82781214073729\n",
            "Iteration : 1531     Loss : 47.80669080234504\n",
            "Iteration : 1532     Loss : 47.7856677946857\n",
            "Iteration : 1533     Loss : 47.7647421678701\n",
            "Iteration : 1534     Loss : 47.743912981492265\n",
            "Iteration : 1535     Loss : 47.72317930453827\n",
            "Iteration : 1536     Loss : 47.702540215296345\n",
            "Iteration : 1537     Loss : 47.68199480126744\n",
            "Iteration : 1538     Loss : 47.66154215907657\n",
            "Iteration : 1539     Loss : 47.64118139438493\n",
            "Iteration : 1540     Loss : 47.6209116218029\n",
            "Iteration : 1541     Loss : 47.60073196480358\n",
            "Iteration : 1542     Loss : 47.58064155563715\n",
            "Iteration : 1543     Loss : 47.56063953524599\n",
            "Iteration : 1544     Loss : 47.540725053180566\n",
            "Iteration : 1545     Loss : 47.52089726751588\n",
            "Iteration : 1546     Loss : 47.50115534476878\n",
            "Iteration : 1547     Loss : 47.481498459816024\n",
            "Iteration : 1548     Loss : 47.46192579581291\n",
            "Iteration : 1549     Loss : 47.44243654411264\n",
            "Iteration : 1550     Loss : 47.42302990418669\n",
            "Iteration : 1551     Loss : 47.40370508354532\n",
            "Iteration : 1552     Loss : 47.38446129765933\n",
            "Iteration : 1553     Loss : 47.365297769882154\n",
            "Iteration : 1554     Loss : 47.34621373137292\n",
            "Iteration : 1555     Loss : 47.327208421019826\n",
            "Iteration : 1556     Loss : 47.308281085364584\n",
            "Iteration : 1557     Loss : 47.28943097852728\n",
            "Iteration : 1558     Loss : 47.27065736213211\n",
            "Iteration : 1559     Loss : 47.25195950523345\n",
            "Iteration : 1560     Loss : 47.23333668424313\n",
            "Iteration : 1561     Loss : 47.21478818285763\n",
            "Iteration : 1562     Loss : 47.19631329198689\n",
            "Iteration : 1563     Loss : 47.17791130968269\n",
            "Iteration : 1564     Loss : 47.15958154106854\n",
            "Iteration : 1565     Loss : 47.141323298269775\n",
            "Iteration : 1566     Loss : 47.1231359003444\n",
            "Iteration : 1567     Loss : 47.10501867321464\n",
            "Iteration : 1568     Loss : 47.086970949598914\n",
            "Iteration : 1569     Loss : 47.068992068944645\n",
            "Iteration : 1570     Loss : 47.05108137736165\n",
            "Iteration : 1571     Loss : 47.033238227555906\n",
            "Iteration : 1572     Loss : 47.01546197876447\n",
            "Iteration : 1573     Loss : 46.997751996690226\n",
            "Iteration : 1574     Loss : 46.980107653438004\n",
            "Iteration : 1575     Loss : 46.962528327450826\n",
            "Iteration : 1576     Loss : 46.94501340344677\n",
            "Iteration : 1577     Loss : 46.92756227235681\n",
            "Iteration : 1578     Loss : 46.910174331262596\n",
            "Iteration : 1579     Loss : 46.89284898333551\n",
            "Iteration : 1580     Loss : 46.875585637775686\n",
            "Iteration : 1581     Loss : 46.85838370975209\n",
            "Iteration : 1582     Loss : 46.84124262034278\n",
            "Iteration : 1583     Loss : 46.82416179647599\n",
            "Iteration : 1584     Loss : 46.807140670871696\n",
            "Iteration : 1585     Loss : 46.79017868198364\n",
            "Iteration : 1586     Loss : 46.77327527394199\n",
            "Iteration : 1587     Loss : 46.75642989649665\n",
            "Iteration : 1588     Loss : 46.73964200496082\n",
            "Iteration : 1589     Loss : 46.722911060155404\n",
            "Iteration : 1590     Loss : 46.70623652835378\n",
            "Iteration : 1591     Loss : 46.68961788122702\n",
            "Iteration : 1592     Loss : 46.67305459578993\n",
            "Iteration : 1593     Loss : 46.65654615434714\n",
            "Iteration : 1594     Loss : 46.64009204444043\n",
            "Iteration : 1595     Loss : 46.62369175879563\n",
            "Iteration : 1596     Loss : 46.60734479527079\n",
            "Iteration : 1597     Loss : 46.59105065680455\n",
            "Iteration : 1598     Loss : 46.574808851365006\n",
            "Iteration : 1599     Loss : 46.55861889189911\n",
            "Iteration : 1600     Loss : 46.542480296282484\n",
            "Iteration : 1601     Loss : 46.52639258726999\n",
            "Iteration : 1602     Loss : 46.51035529244619\n",
            "Iteration : 1603     Loss : 46.49436794417716\n",
            "Iteration : 1604     Loss : 46.47843007956177\n",
            "Iteration : 1605     Loss : 46.46254124038439\n",
            "Iteration : 1606     Loss : 46.446700973067294\n",
            "Iteration : 1607     Loss : 46.43090882862407\n",
            "Iteration : 1608     Loss : 46.41516436261307\n",
            "Iteration : 1609     Loss : 46.39946713509165\n",
            "Iteration : 1610     Loss : 46.38381671057064\n",
            "Iteration : 1611     Loss : 46.3682126579693\n",
            "Iteration : 1612     Loss : 46.352654550570925\n",
            "Iteration : 1613     Loss : 46.33714196597847\n",
            "Iteration : 1614     Loss : 46.321674486070904\n",
            "Iteration : 1615     Loss : 46.30625169696015\n",
            "Iteration : 1616     Loss : 46.290873188947955\n",
            "Iteration : 1617     Loss : 46.27553855648369\n",
            "Iteration : 1618     Loss : 46.260247398122026\n",
            "Iteration : 1619     Loss : 46.24499931648179\n",
            "Iteration : 1620     Loss : 46.229793918204344\n",
            "Iteration : 1621     Loss : 46.21463081391312\n",
            "Iteration : 1622     Loss : 46.19950961817309\n",
            "Iteration : 1623     Loss : 46.18442994945075\n",
            "Iteration : 1624     Loss : 46.16939143007474\n",
            "Iteration : 1625     Loss : 46.15439368619641\n",
            "Iteration : 1626     Loss : 46.13943634775117\n",
            "Iteration : 1627     Loss : 46.12451904842005\n",
            "Iteration : 1628     Loss : 46.10964142559166\n",
            "Iteration : 1629     Loss : 46.09480312032449\n",
            "Iteration : 1630     Loss : 46.08000377730978\n",
            "Iteration : 1631     Loss : 46.065243044834304\n",
            "Iteration : 1632     Loss : 46.05052057474412\n",
            "Iteration : 1633     Loss : 46.035836022408354\n",
            "Iteration : 1634     Loss : 46.02118904668323\n",
            "Iteration : 1635     Loss : 46.00657930987674\n",
            "Iteration : 1636     Loss : 45.99200647771344\n",
            "Iteration : 1637     Loss : 45.977470219299796\n",
            "Iteration : 1638     Loss : 45.962970207089676\n",
            "Iteration : 1639     Loss : 45.9485061168504\n",
            "Iteration : 1640     Loss : 45.93407762762886\n",
            "Iteration : 1641     Loss : 45.919684421718365\n",
            "Iteration : 1642     Loss : 45.90532618462529\n",
            "Iteration : 1643     Loss : 45.89100260503662\n",
            "Iteration : 1644     Loss : 45.87671337478739\n",
            "Iteration : 1645     Loss : 45.86245818882876\n",
            "Iteration : 1646     Loss : 45.848236745196154\n",
            "Iteration : 1647     Loss : 45.83404874497783\n",
            "Iteration : 1648     Loss : 45.819893892283886\n",
            "Iteration : 1649     Loss : 45.80577189421539\n",
            "Iteration : 1650     Loss : 45.791682460833826\n",
            "Iteration : 1651     Loss : 45.77762530513108\n",
            "Iteration : 1652     Loss : 45.763600142999344\n",
            "Iteration : 1653     Loss : 45.749606693201876\n",
            "Iteration : 1654     Loss : 45.73564467734335\n",
            "Iteration : 1655     Loss : 45.72171381984107\n",
            "Iteration : 1656     Loss : 45.70781384789631\n",
            "Iteration : 1657     Loss : 45.69394449146582\n",
            "Iteration : 1658     Loss : 45.68010548323379\n",
            "Iteration : 1659     Loss : 45.666296558584\n",
            "Iteration : 1660     Loss : 45.65251745557221\n",
            "Iteration : 1661     Loss : 45.638767914899034\n",
            "Iteration : 1662     Loss : 45.625047679882826\n",
            "Iteration : 1663     Loss : 45.611356496432975\n",
            "Iteration : 1664     Loss : 45.59769411302358\n",
            "Iteration : 1665     Loss : 45.58406028066711\n",
            "Iteration : 1666     Loss : 45.57045475288855\n",
            "Iteration : 1667     Loss : 45.55687728569984\n",
            "Iteration : 1668     Loss : 45.543327637574336\n",
            "Iteration : 1669     Loss : 45.529805569421804\n",
            "Iteration : 1670     Loss : 45.51631084456344\n",
            "Iteration : 1671     Loss : 45.50284322870739\n",
            "Iteration : 1672     Loss : 45.48940248992419\n",
            "Iteration : 1673     Loss : 45.47598839862272\n",
            "Iteration : 1674     Loss : 45.462600727526414\n",
            "Iteration : 1675     Loss : 45.4492392516495\n",
            "Iteration : 1676     Loss : 45.43590374827373\n",
            "Iteration : 1677     Loss : 45.422593996924945\n",
            "Iteration : 1678     Loss : 45.40930977935054\n",
            "Iteration : 1679     Loss : 45.396050879496485\n",
            "Iteration : 1680     Loss : 45.38281708348503\n",
            "Iteration : 1681     Loss : 45.369608179592255\n",
            "Iteration : 1682     Loss : 45.35642395822639\n",
            "Iteration : 1683     Loss : 45.34326421190573\n",
            "Iteration : 1684     Loss : 45.330128735237295\n",
            "Iteration : 1685     Loss : 45.31701732489534\n",
            "Iteration : 1686     Loss : 45.303929779600345\n",
            "Iteration : 1687     Loss : 45.29086590009806\n",
            "Iteration : 1688     Loss : 45.27782548913881\n",
            "Iteration : 1689     Loss : 45.26480835145702\n",
            "Iteration : 1690     Loss : 45.25181429375099\n",
            "Iteration : 1691     Loss : 45.238843124662715\n",
            "Iteration : 1692     Loss : 45.22589465475822\n",
            "Iteration : 1693     Loss : 45.21296869650767\n",
            "Iteration : 1694     Loss : 45.20006506426606\n",
            "Iteration : 1695     Loss : 45.18718357425399\n",
            "Iteration : 1696     Loss : 45.174324044538565\n",
            "Iteration : 1697     Loss : 45.16148629501436\n",
            "Iteration : 1698     Loss : 45.14867014738506\n",
            "Iteration : 1699     Loss : 45.135875425144825\n",
            "Iteration : 1700     Loss : 45.12310195355984\n",
            "Iteration : 1701     Loss : 45.11034955965053\n",
            "Iteration : 1702     Loss : 45.097618072173404\n",
            "Iteration : 1703     Loss : 45.08490732160341\n",
            "Iteration : 1704     Loss : 45.072217140116344\n",
            "Iteration : 1705     Loss : 45.05954736157156\n",
            "Iteration : 1706     Loss : 45.04689782149466\n",
            "Iteration : 1707     Loss : 45.03426835706055\n",
            "Iteration : 1708     Loss : 45.021658807076534\n",
            "Iteration : 1709     Loss : 45.00906901196576\n",
            "Iteration : 1710     Loss : 44.99649881375052\n",
            "Iteration : 1711     Loss : 44.98394805603615\n",
            "Iteration : 1712     Loss : 44.97141658399465\n",
            "Iteration : 1713     Loss : 44.958904244348886\n",
            "Iteration : 1714     Loss : 44.946410885356535\n",
            "Iteration : 1715     Loss : 44.93393635679461\n",
            "Iteration : 1716     Loss : 44.92148050994387\n",
            "Iteration : 1717     Loss : 44.9090431975734\n",
            "Iteration : 1718     Loss : 44.896624273925504\n",
            "Iteration : 1719     Loss : 44.88422359470069\n",
            "Iteration : 1720     Loss : 44.87184101704262\n",
            "Iteration : 1721     Loss : 44.85947639952358\n",
            "Iteration : 1722     Loss : 44.8471296021298\n",
            "Iteration : 1723     Loss : 44.83480048624692\n",
            "Iteration : 1724     Loss : 44.82248891464598\n",
            "Iteration : 1725     Loss : 44.81019475146898\n",
            "Iteration : 1726     Loss : 44.79791786221513\n",
            "Iteration : 1727     Loss : 44.78565811372682\n",
            "Iteration : 1728     Loss : 44.773415374175954\n",
            "Iteration : 1729     Loss : 44.7611895130505\n",
            "Iteration : 1730     Loss : 44.748980401140955\n",
            "Iteration : 1731     Loss : 44.73678791052708\n",
            "Iteration : 1732     Loss : 44.72461191456471\n",
            "Iteration : 1733     Loss : 44.71245228787283\n",
            "Iteration : 1734     Loss : 44.70030890632058\n",
            "Iteration : 1735     Loss : 44.68818164701463\n",
            "Iteration : 1736     Loss : 44.67607038828643\n",
            "Iteration : 1737     Loss : 44.66397500967978\n",
            "Iteration : 1738     Loss : 44.65189539193849\n",
            "Iteration : 1739     Loss : 44.63983141699411\n",
            "Iteration : 1740     Loss : 44.62778296795383\n",
            "Iteration : 1741     Loss : 44.61574992908849\n",
            "Iteration : 1742     Loss : 44.603732185820704\n",
            "Iteration : 1743     Loss : 44.59172962471319\n",
            "Iteration : 1744     Loss : 44.57974213345702\n",
            "Iteration : 1745     Loss : 44.5677696008602\n",
            "Iteration : 1746     Loss : 44.55581191683639\n",
            "Iteration : 1747     Loss : 44.54386897239342\n",
            "Iteration : 1748     Loss : 44.531940659622265\n",
            "Iteration : 1749     Loss : 44.52002687168602\n",
            "Iteration : 1750     Loss : 44.50812750280896\n",
            "Iteration : 1751     Loss : 44.4962424482657\n",
            "Iteration : 1752     Loss : 44.48437160437052\n",
            "Iteration : 1753     Loss : 44.47251486846679\n",
            "Iteration : 1754     Loss : 44.46067213891643\n",
            "Iteration : 1755     Loss : 44.44884331508966\n",
            "Iteration : 1756     Loss : 44.43702829735459\n",
            "Iteration : 1757     Loss : 44.425226987067155\n",
            "Iteration : 1758     Loss : 44.413439286560966\n",
            "Iteration : 1759     Loss : 44.401665099137524\n",
            "Iteration : 1760     Loss : 44.38990432905626\n",
            "Iteration : 1761     Loss : 44.37815688152467\n",
            "Iteration : 1762     Loss : 44.366422662688876\n",
            "Iteration : 1763     Loss : 44.35470157962392\n",
            "Iteration : 1764     Loss : 44.342993540324514\n",
            "Iteration : 1765     Loss : 44.331298453695425\n",
            "Iteration : 1766     Loss : 44.31961622954237\n",
            "Iteration : 1767     Loss : 44.30794677856282\n",
            "Iteration : 1768     Loss : 44.29629001233699\n",
            "Iteration : 1769     Loss : 44.28464584331869\n",
            "Iteration : 1770     Loss : 44.27301418482665\n",
            "Iteration : 1771     Loss : 44.261394951035584\n",
            "Iteration : 1772     Loss : 44.24978805696758\n",
            "Iteration : 1773     Loss : 44.238193418483355\n",
            "Iteration : 1774     Loss : 44.226610952273916\n",
            "Iteration : 1775     Loss : 44.21504057585196\n",
            "Iteration : 1776     Loss : 44.20348220754363\n",
            "Iteration : 1777     Loss : 44.191935766480086\n",
            "Iteration : 1778     Loss : 44.18040117258963\n",
            "Iteration : 1779     Loss : 44.16887834658929\n",
            "Iteration : 1780     Loss : 44.15736720997696\n",
            "Iteration : 1781     Loss : 44.14586768502349\n",
            "Iteration : 1782     Loss : 44.13437969476473\n",
            "Iteration : 1783     Loss : 44.1229031629939\n",
            "Iteration : 1784     Loss : 44.11143801425374\n",
            "Iteration : 1785     Loss : 44.09998417382908\n",
            "Iteration : 1786     Loss : 44.08854156773915\n",
            "Iteration : 1787     Loss : 44.07711012273017\n",
            "Iteration : 1788     Loss : 44.06568976626806\n",
            "Iteration : 1789     Loss : 44.05428042653105\n",
            "Iteration : 1790     Loss : 44.042882032402375\n",
            "Iteration : 1791     Loss : 44.0314945134633\n",
            "Iteration : 1792     Loss : 44.020117799986004\n",
            "Iteration : 1793     Loss : 44.00875182292637\n",
            "Iteration : 1794     Loss : 43.997396513917366\n",
            "Iteration : 1795     Loss : 43.98605180526185\n",
            "Iteration : 1796     Loss : 43.974717629926104\n",
            "Iteration : 1797     Loss : 43.963393921532806\n",
            "Iteration : 1798     Loss : 43.95208061435463\n",
            "Iteration : 1799     Loss : 43.94077764330741\n",
            "Iteration : 1800     Loss : 43.92948494394388\n",
            "Iteration : 1801     Loss : 43.91820245244708\n",
            "Iteration : 1802     Loss : 43.90693010562384\n",
            "Iteration : 1803     Loss : 43.895667840898795\n",
            "Iteration : 1804     Loss : 43.88441559630776\n",
            "Iteration : 1805     Loss : 43.873173310491836\n",
            "Iteration : 1806     Loss : 43.8619409226911\n",
            "Iteration : 1807     Loss : 43.85071837273859\n",
            "Iteration : 1808     Loss : 43.839505601054356\n",
            "Iteration : 1809     Loss : 43.82830254863937\n",
            "Iteration : 1810     Loss : 43.817109157069794\n",
            "Iteration : 1811     Loss : 43.805925368491145\n",
            "Iteration : 1812     Loss : 43.79475112561238\n",
            "Iteration : 1813     Loss : 43.783586371700466\n",
            "Iteration : 1814     Loss : 43.77243105057441\n",
            "Iteration : 1815     Loss : 43.76128510659989\n",
            "Iteration : 1816     Loss : 43.750148484683784\n",
            "Iteration : 1817     Loss : 43.73902113026835\n",
            "Iteration : 1818     Loss : 43.727902989326296\n",
            "Iteration : 1819     Loss : 43.71679400835496\n",
            "Iteration : 1820     Loss : 43.705694134371356\n",
            "Iteration : 1821     Loss : 43.69460331490667\n",
            "Iteration : 1822     Loss : 43.683521498001234\n",
            "Iteration : 1823     Loss : 43.67244863219926\n",
            "Iteration : 1824     Loss : 43.66138466654385\n",
            "Iteration : 1825     Loss : 43.65032955057183\n",
            "Iteration : 1826     Loss : 43.63928323430893\n",
            "Iteration : 1827     Loss : 43.62824566826467\n",
            "Iteration : 1828     Loss : 43.61721680342759\n",
            "Iteration : 1829     Loss : 43.60619659126038\n",
            "Iteration : 1830     Loss : 43.59518498369507\n",
            "Iteration : 1831     Loss : 43.58418193312828\n",
            "Iteration : 1832     Loss : 43.5731873924166\n",
            "Iteration : 1833     Loss : 43.56220131487193\n",
            "Iteration : 1834     Loss : 43.55122365425679\n",
            "Iteration : 1835     Loss : 43.540254364779855\n",
            "Iteration : 1836     Loss : 43.529293401091444\n",
            "Iteration : 1837     Loss : 43.51834071827907\n",
            "Iteration : 1838     Loss : 43.50739627186305\n",
            "Iteration : 1839     Loss : 43.49646001779204\n",
            "Iteration : 1840     Loss : 43.48553191243873\n",
            "Iteration : 1841     Loss : 43.4746119125958\n",
            "Iteration : 1842     Loss : 43.463699975471314\n",
            "Iteration : 1843     Loss : 43.45279605868487\n",
            "Iteration : 1844     Loss : 43.441900120263185\n",
            "Iteration : 1845     Loss : 43.43101211863617\n",
            "Iteration : 1846     Loss : 43.42013201263281\n",
            "Iteration : 1847     Loss : 43.409259761477074\n",
            "Iteration : 1848     Loss : 43.39839532478412\n",
            "Iteration : 1849     Loss : 43.38753866255611\n",
            "Iteration : 1850     Loss : 43.37668973517847\n",
            "Iteration : 1851     Loss : 43.36584850341608\n",
            "Iteration : 1852     Loss : 43.35501492840922\n",
            "Iteration : 1853     Loss : 43.34418897167006\n",
            "Iteration : 1854     Loss : 43.333370595078726\n",
            "Iteration : 1855     Loss : 43.32255976087968\n",
            "Iteration : 1856     Loss : 43.31175643167805\n",
            "Iteration : 1857     Loss : 43.300960570436004\n",
            "Iteration : 1858     Loss : 43.29017214046906\n",
            "Iteration : 1859     Loss : 43.27939110544267\n",
            "Iteration : 1860     Loss : 43.26861742936867\n",
            "Iteration : 1861     Loss : 43.257851076601696\n",
            "Iteration : 1862     Loss : 43.24709201183586\n",
            "Iteration : 1863     Loss : 43.23634020010124\n",
            "Iteration : 1864     Loss : 43.2255956067606\n",
            "Iteration : 1865     Loss : 43.21485819750591\n",
            "Iteration : 1866     Loss : 43.2041279383552\n",
            "Iteration : 1867     Loss : 43.193404795649116\n",
            "Iteration : 1868     Loss : 43.182688736047865\n",
            "Iteration : 1869     Loss : 43.171979726527816\n",
            "Iteration : 1870     Loss : 43.16127773437839\n",
            "Iteration : 1871     Loss : 43.15058272719904\n",
            "Iteration : 1872     Loss : 43.13989467289595\n",
            "Iteration : 1873     Loss : 43.12921353967903\n",
            "Iteration : 1874     Loss : 43.118539296058906\n",
            "Iteration : 1875     Loss : 43.10787191084387\n",
            "Iteration : 1876     Loss : 43.0972113531368\n",
            "Iteration : 1877     Loss : 43.08655759233247\n",
            "Iteration : 1878     Loss : 43.07591059811423\n",
            "Iteration : 1879     Loss : 43.0652703404515\n",
            "Iteration : 1880     Loss : 43.05463678959658\n",
            "Iteration : 1881     Loss : 43.044009916082075\n",
            "Iteration : 1882     Loss : 43.033389690717826\n",
            "Iteration : 1883     Loss : 43.02277608458841\n",
            "Iteration : 1884     Loss : 43.01216906905017\n",
            "Iteration : 1885     Loss : 43.00156861572853\n",
            "Iteration : 1886     Loss : 42.9909746965154\n",
            "Iteration : 1887     Loss : 42.98038728356633\n",
            "Iteration : 1888     Loss : 42.96980634929808\n",
            "Iteration : 1889     Loss : 42.95923186638579\n",
            "Iteration : 1890     Loss : 42.94866380776063\n",
            "Iteration : 1891     Loss : 42.93810214660695\n",
            "Iteration : 1892     Loss : 42.927546856359946\n",
            "Iteration : 1893     Loss : 42.916997910703174\n",
            "Iteration : 1894     Loss : 42.90645528356587\n",
            "Iteration : 1895     Loss : 42.89591894912064\n",
            "Iteration : 1896     Loss : 42.88538888178094\n",
            "Iteration : 1897     Loss : 42.87486505619886\n",
            "Iteration : 1898     Loss : 42.864347447262446\n",
            "Iteration : 1899     Loss : 42.85383603009348\n",
            "Iteration : 1900     Loss : 42.84333078004522\n",
            "Iteration : 1901     Loss : 42.832831672699946\n",
            "Iteration : 1902     Loss : 42.82233868386677\n",
            "Iteration : 1903     Loss : 42.81185178957928\n",
            "Iteration : 1904     Loss : 42.80137096609333\n",
            "Iteration : 1905     Loss : 42.79089618988489\n",
            "Iteration : 1906     Loss : 42.78042743764773\n",
            "Iteration : 1907     Loss : 42.769964686291324\n",
            "Iteration : 1908     Loss : 42.75950791293861\n",
            "Iteration : 1909     Loss : 42.74905709492397\n",
            "Iteration : 1910     Loss : 42.73861220979104\n",
            "Iteration : 1911     Loss : 42.728173235290676\n",
            "Iteration : 1912     Loss : 42.71774014937876\n",
            "Iteration : 1913     Loss : 42.70731293021428\n",
            "Iteration : 1914     Loss : 42.69689155615727\n",
            "Iteration : 1915     Loss : 42.68647600576679\n",
            "Iteration : 1916     Loss : 42.67606625779891\n",
            "Iteration : 1917     Loss : 42.66566229120483\n",
            "Iteration : 1918     Loss : 42.65526408512882\n",
            "Iteration : 1919     Loss : 42.64487161890641\n",
            "Iteration : 1920     Loss : 42.6344848720623\n",
            "Iteration : 1921     Loss : 42.624103824308726\n",
            "Iteration : 1922     Loss : 42.61372845554343\n",
            "Iteration : 1923     Loss : 42.60335874584774\n",
            "Iteration : 1924     Loss : 42.59299467548498\n",
            "Iteration : 1925     Loss : 42.58263622489834\n",
            "Iteration : 1926     Loss : 42.5722833747094\n",
            "Iteration : 1927     Loss : 42.56193610571619\n",
            "Iteration : 1928     Loss : 42.551594398891325\n",
            "Iteration : 1929     Loss : 42.54125823538052\n",
            "Iteration : 1930     Loss : 42.53092759650065\n",
            "Iteration : 1931     Loss : 42.52060246373816\n",
            "Iteration : 1932     Loss : 42.51028281874733\n",
            "Iteration : 1933     Loss : 42.49996864334857\n",
            "Iteration : 1934     Loss : 42.489659919526936\n",
            "Iteration : 1935     Loss : 42.479356629430214\n",
            "Iteration : 1936     Loss : 42.46905875536753\n",
            "Iteration : 1937     Loss : 42.45876627980766\n",
            "Iteration : 1938     Loss : 42.44847918537745\n",
            "Iteration : 1939     Loss : 42.43819745486021\n",
            "Iteration : 1940     Loss : 42.42792107119418\n",
            "Iteration : 1941     Loss : 42.41765001747101\n",
            "Iteration : 1942     Loss : 42.407384276934245\n",
            "Iteration : 1943     Loss : 42.39712383297771\n",
            "Iteration : 1944     Loss : 42.38686866914419\n",
            "Iteration : 1945     Loss : 42.37661876912376\n",
            "Iteration : 1946     Loss : 42.36637411675246\n",
            "Iteration : 1947     Loss : 42.35613469601077\n",
            "Iteration : 1948     Loss : 42.34590049102222\n",
            "Iteration : 1949     Loss : 42.335671486051936\n",
            "Iteration : 1950     Loss : 42.32544766550526\n",
            "Iteration : 1951     Loss : 42.31522901392632\n",
            "Iteration : 1952     Loss : 42.305015515996665\n",
            "Iteration : 1953     Loss : 42.29480715653393\n",
            "Iteration : 1954     Loss : 42.28460392049047\n",
            "Iteration : 1955     Loss : 42.274405792952\n",
            "Iteration : 1956     Loss : 42.264212759136264\n",
            "Iteration : 1957     Loss : 42.2540248043918\n",
            "Iteration : 1958     Loss : 42.243841914196544\n",
            "Iteration : 1959     Loss : 42.23366407415657\n",
            "Iteration : 1960     Loss : 42.22349127000492\n",
            "Iteration : 1961     Loss : 42.213323487600185\n",
            "Iteration : 1962     Loss : 42.20316071292536\n",
            "Iteration : 1963     Loss : 42.19300293208663\n",
            "Iteration : 1964     Loss : 42.182850131312044\n",
            "Iteration : 1965     Loss : 42.172702296950376\n",
            "Iteration : 1966     Loss : 42.16255941546995\n",
            "Iteration : 1967     Loss : 42.15242147345742\n",
            "Iteration : 1968     Loss : 42.142288457616644\n",
            "Iteration : 1969     Loss : 42.13216035476738\n",
            "Iteration : 1970     Loss : 42.12203715184431\n",
            "Iteration : 1971     Loss : 42.111918835895814\n",
            "Iteration : 1972     Loss : 42.1018053940829\n",
            "Iteration : 1973     Loss : 42.09169681367796\n",
            "Iteration : 1974     Loss : 42.08159308206386\n",
            "Iteration : 1975     Loss : 42.071494186732686\n",
            "Iteration : 1976     Loss : 42.061400115284755\n",
            "Iteration : 1977     Loss : 42.05131085542741\n",
            "Iteration : 1978     Loss : 42.041226394974224\n",
            "Iteration : 1979     Loss : 42.03114672184368\n",
            "Iteration : 1980     Loss : 42.02107182405816\n",
            "Iteration : 1981     Loss : 42.01100168974322\n",
            "Iteration : 1982     Loss : 42.00093630712614\n",
            "Iteration : 1983     Loss : 41.99087566453522\n",
            "Iteration : 1984     Loss : 41.98081975039869\n",
            "Iteration : 1985     Loss : 41.970768553243666\n",
            "Iteration : 1986     Loss : 41.96072206169524\n",
            "Iteration : 1987     Loss : 41.95068026447555\n",
            "Iteration : 1988     Loss : 41.940643150402664\n",
            "Iteration : 1989     Loss : 41.93061070838984\n",
            "Iteration : 1990     Loss : 41.92058292744443\n",
            "Iteration : 1991     Loss : 41.910559796667\n",
            "Iteration : 1992     Loss : 41.900541305250385\n",
            "Iteration : 1993     Loss : 41.8905274424789\n",
            "Iteration : 1994     Loss : 41.88051819772722\n",
            "Iteration : 1995     Loss : 41.8705135604597\n",
            "Iteration : 1996     Loss : 41.860513520229425\n",
            "Iteration : 1997     Loss : 41.85051806667715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi5rMn_cYGll",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2cc67082-a9c5-4008-f8c2-b265581a54c7"
      },
      "source": [
        "lg.acc()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy = 97.5 \n",
            " Test Accuracy = 100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt64WEN_bnVK",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression using SKlearn library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DaM9B6BYHTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FVvpDmLdXbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n=len(trn.columns)\n",
        "attr=list(trn.columns[0:n-1])\n",
        "trnx=trn[attr]\n",
        "trny=trn[trn.columns[-1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVGoxTkJYHg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = LogisticRegression(random_state=0).fit(trnx, trny)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akSRY2rFYHGI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9d83a5c-20f5-4077-dce9-927404bae53e"
      },
      "source": [
        "clf.score(trnx,trny)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7eeOqYdYGIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}